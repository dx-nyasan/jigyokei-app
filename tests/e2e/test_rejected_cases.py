"""
E2E Test: Rejected Cases Quality Verification
Tests the certification architecture against actual rejected application cases.
"""
import sys
import os

# Add project root to path
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.insert(0, PROJECT_ROOT)


# PDF extraction
try:
    import fitz  # pymupdf
except ImportError:
    print("ERROR: pymupdf is required. Install with: pip install pymupdf")
    sys.exit(1)

from src.api.schemas import ApplicationRoot
from src.core.completion_checker import CompletionChecker
from src.core.certification_requirements import requirements_loader
from src.core.audit_agent import AuditAgent


REAP_REPORT_DIR = r"C:\Users\kitahara\Desktop\script\jigyokei-copilot\reap report"


def extract_pdf_text(filename: str) -> str:
    """Extract text from PDF file."""
    filepath = os.path.join(REAP_REPORT_DIR, filename)
    if not os.path.exists(filepath):
        return f"[FILE NOT FOUND: {filename}]"
    
    doc = fitz.open(filepath)
    text_parts = []
    for page in doc:
        text_parts.append(page.get_text())
    doc.close()
    return "\n".join(text_parts)


def analyze_case(case_name: str, pdf_files: list, expected_issues: list) -> dict:
    """
    Analyze a case and check for expected issues.
    
    Returns dict with:
    - case_name
    - total_score
    - detected_issues
    - expected_issues_found
    - pass_status
    """
    print(f"\n{'='*60}")
    print(f"ã€{case_name}ã€‘")
    print(f"{'='*60}")
    
    # Extract text from PDFs
    combined_text = ""
    for pdf in pdf_files:
        print(f"  Extracting: {pdf}")
        text = extract_pdf_text(pdf)
        combined_text += f"\n\n--- {pdf} ---\n{text}"
    
    print(f"  Total text length: {len(combined_text)} characters")
    
    # Check keyword presence using certification requirements
    print("\n  ã€èªå®šè¦ä»¶ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ã€‘")
    sections_to_check = ["disaster_assumption", "business_impact", "pdca"]
    keyword_results = {}
    
    for section_id in sections_to_check:
        keywords = requirements_loader.check_keywords(section_id, combined_text)
        section = requirements_loader.get_section(section_id)
        found_count = sum(keywords.values())
        total_count = len(keywords)
        
        print(f"    {section.name}: {found_count}/{total_count} keywords found")
        keyword_results[section_id] = {
            "found": found_count,
            "total": total_count,
            "missing": [k for k, v in keywords.items() if not v]
        }
    
    # Calculate certification score
    total_score = 0
    for section_id in sections_to_check:
        score = requirements_loader.calculate_section_score(section_id, combined_text)
        total_score += score
    
    print(f"\n  ã€èªå®šã‚¹ã‚³ã‚¢ï¼ˆæ¨å®šï¼‰ã€‘: {total_score}/55ç‚¹")
    
    # Check for expected issues
    print("\n  ã€ä¸å‚™æŒ‡æ‘˜é …ç›®ã®æ¤œçŸ¥ã€‘")
    detected_issues = []
    
    for section_id, results in keyword_results.items():
        if results["missing"]:
            issue = f"{requirements_loader.get_section(section_id).name}: {', '.join(results['missing'])} ãŒä¸è¶³"
            detected_issues.append(issue)
            print(f"    âš ï¸ {issue}")
    
    if not detected_issues:
        print("    âœ… ä¸å‚™ãªã—")
    
    # Check if expected issues are detected
    expected_found = []
    for expected in expected_issues:
        for detected in detected_issues:
            if expected.lower() in detected.lower():
                expected_found.append(expected)
                break
    
    pass_status = len(expected_found) == len(expected_issues)
    
    return {
        "case_name": case_name,
        "total_score": total_score,
        "keyword_results": keyword_results,
        "detected_issues": detected_issues,
        "expected_issues": expected_issues,
        "expected_found": expected_found,
        "pass_status": pass_status
    }


def test_case1_approved():
    """Test Case 1: Originally approved application."""
    print("\n" + "="*70)
    print("ãƒ†ã‚¹ãƒˆ1: æ¡ˆä»¶1ï¼ˆèªå®šæ¸ˆã¿ï¼‰- æ­£å¸¸ã‚±ãƒ¼ã‚¹ã¨ã—ã¦ä¸å‚™ãŒå‡ºãªã„ã“ã¨ã‚’ç¢ºèª")
    print("="*70)
    
    result = analyze_case(
        "æ¡ˆä»¶1ï¼ˆèªå®šæ¸ˆã¿ï¼‰",
        [
            "æ¡ˆä»¶1_äº‹æ¥­ç¶™ç¶šåŠ›å¼·åŒ–è¨ˆç”»ç”³è«‹æ›¸.pdf",
            "æ¡ˆä»¶1_é›»å­ç”³è«‹ä¸‹æ›¸ãã‚·ãƒ¼ãƒˆã€€äº‹æ¥­ç¶™ç¶šåŠ›å¼·åŒ–è¨ˆç”».pdf"
        ],
        expected_issues=[]  # No issues expected for approved case
    )
    
    # Approved case should have high score and no critical issues
    if result["total_score"] >= 30 and len(result["detected_issues"]) <= 2:
        print("\nâœ… PASS: æ¡ˆä»¶1ã¯èªå®šãƒ¬ãƒ™ãƒ«ã®å“è³ªã§ã™")
        return True
    else:
        print(f"\nâš ï¸ WARNING: æ¡ˆä»¶1ã®ã‚¹ã‚³ã‚¢ãŒä½ã„ ({result['total_score']}ç‚¹)")
        return False


def test_case2_initial_rejected():
    """Test Case 2 Initial: Should detect issues that led to rejection."""
    print("\n" + "="*70)
    print("ãƒ†ã‚¹ãƒˆ2: æ¡ˆä»¶2ï¼ˆåˆå›ç”³è«‹ï¼‰- ä¸å‚™æŒ‡æ‘˜ã•ã‚ŒãŸå†…å®¹ã‚’æ¤œçŸ¥ã§ãã‚‹ã‹ç¢ºèª")
    print("="*70)
    
    result = analyze_case(
        "æ¡ˆä»¶2ï¼ˆåˆå›ç”³è«‹ - ä¸å‚™æŒ‡æ‘˜å‰ï¼‰",
        [
            "æ¡ˆä»¶2_071215ç”³è«‹å†…å®¹.pdf"
        ],
        expected_issues=[
            "éœ‡åº¦",  # Missing specific disaster assumption
            "ç¢ºç‡"   # Missing probability data
        ]
    )
    
    # Initial rejected version should have low score and issues detected
    if result["total_score"] < 40 or len(result["detected_issues"]) >= 1:
        print(f"\nâœ… PASS: æ¡ˆä»¶2ï¼ˆåˆå›ï¼‰ã®ä¸å‚™ã‚’æ­£ã—ãæ¤œçŸ¥ã—ã¾ã—ãŸ")
        print(f"   æ¤œçŸ¥ã•ã‚ŒãŸä¸å‚™: {result['detected_issues']}")
        return True
    else:
        print(f"\nâŒ FAIL: æ¡ˆä»¶2ï¼ˆåˆå›ï¼‰ã®ä¸å‚™ã‚’æ¤œçŸ¥ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return False


def test_case2_after_revision():
    """Test Case 2 After Revision: Should have fewer/no issues."""
    print("\n" + "="*70)
    print("ãƒ†ã‚¹ãƒˆ3: æ¡ˆä»¶2ï¼ˆä¿®æ­£å¾Œï¼‰- ä¿®æ­£ã«ã‚ˆã‚Šå“è³ªãŒå‘ä¸Šã—ãŸã“ã¨ã‚’ç¢ºèª")
    print("="*70)
    
    result = analyze_case(
        "æ¡ˆä»¶2ï¼ˆä¿®æ­£å¾Œ - èªå®šï¼‰",
        [
            "æ¡ˆä»¶2_ä¸å‚™æŒ‡æ‘˜å¾Œ_ä¿®æ­£ç”³è«‹.pdf"
        ],
        expected_issues=[]  # Should be fixed
    )
    
    # Revised version should have higher score
    if result["total_score"] >= 30:
        print(f"\nâœ… PASS: æ¡ˆä»¶2ï¼ˆä¿®æ­£å¾Œï¼‰ã¯èªå®šãƒ¬ãƒ™ãƒ«ã«é”ã—ã¦ã„ã¾ã™")
        return True
    else:
        print(f"\nâš ï¸ WARNING: æ¡ˆä»¶2ï¼ˆä¿®æ­£å¾Œï¼‰ã®ã‚¹ã‚³ã‚¢ãŒã¾ã ä½ã„ ({result['total_score']}ç‚¹)")
        return False


def test_completion_checker_integration():
    """Test CompletionChecker with simulated bad/good data."""
    print("\n" + "="*70)
    print("ãƒ†ã‚¹ãƒˆ4: CompletionChecker 12/17æ”¹ä¿®å¯¾å¿œãƒ†ã‚¹ãƒˆ")
    print("="*70)
    
    # Bad case: Missing 12/17 mandatory fields
    bad_plan = ApplicationRoot()
    bad_plan.pdca.training_education = "è¨“ç·´ã‚’å®Ÿæ–½ã™ã‚‹"
    # Missing: training_month, review_month, internal_publicity
    
    bad_result = CompletionChecker.analyze(bad_plan)
    bad_pdca_issues = [m for m in bad_result['missing_mandatory'] if m['section'] == 'PDCA']
    
    print(f"  ä¸å®Œå…¨ãªè¨ˆç”»ã®PDCAä¸å‚™: {len(bad_pdca_issues)}ä»¶")
    for issue in bad_pdca_issues:
        print(f"    - {issue['msg']}")
    
    # Good case: All 12/17 fields filled
    good_plan = ApplicationRoot()
    good_plan.pdca.training_education = "æ•™è‚²åŠã³è¨“ç·´ã‚’å®Ÿæ–½ã™ã‚‹"
    good_plan.pdca.training_month = 2
    good_plan.pdca.review_month = 3
    good_plan.pdca.internal_publicity = "ç¤¾å†…ãƒãƒ¼ã‚¿ãƒ«ã§å‘¨çŸ¥"
    
    good_result = CompletionChecker.analyze(good_plan)
    good_pdca_issues = [m for m in good_result['missing_mandatory'] if m['section'] == 'PDCA']
    
    print(f"\n  å®Œå…¨ãªè¨ˆç”»ã®PDCAä¸å‚™: {len(good_pdca_issues)}ä»¶")
    
    if len(bad_pdca_issues) >= 3 and len(good_pdca_issues) == 0:
        print("\nâœ… PASS: 12/17æ”¹ä¿®å¯¾å¿œã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãŒæ­£å¸¸ã«å‹•ä½œ")
        return True
    else:
        print("\nâŒ FAIL: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã«å•é¡Œã‚ã‚Š")
        return False


def main():
    print("="*70)
    print("E2Eå“è³ªæ¤œè¨¼ãƒ†ã‚¹ãƒˆ: ä¸å‚™æŒ‡æ‘˜æ¡ˆä»¶ã®æ­£å¸¸åŒ–ç¢ºèª")
    print("="*70)
    
    results = []
    
    # Run all tests
    results.append(("æ¡ˆä»¶1ï¼ˆèªå®šæ¸ˆã¿ï¼‰ç¢ºèª", test_case1_approved()))
    results.append(("æ¡ˆä»¶2ï¼ˆåˆå›ç”³è«‹ï¼‰ä¸å‚™æ¤œçŸ¥", test_case2_initial_rejected()))
    results.append(("æ¡ˆä»¶2ï¼ˆä¿®æ­£å¾Œï¼‰å“è³ªç¢ºèª", test_case2_after_revision()))
    results.append(("12/17æ”¹ä¿®å¯¾å¿œãƒ†ã‚¹ãƒˆ", test_completion_checker_integration()))
    
    # Summary
    print("\n" + "="*70)
    print("ãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("="*70)
    
    passed = 0
    failed = 0
    for name, result in results:
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"  {status}: {name}")
        if result:
            passed += 1
        else:
            failed += 1
    
    print(f"\nç·åˆçµæœ: {passed}/{len(results)} ãƒ†ã‚¹ãƒˆæˆåŠŸ")
    
    if failed == 0:
        print("\nğŸ‰ ã™ã¹ã¦ã®E2Eãƒ†ã‚¹ãƒˆã«åˆæ ¼ã—ã¾ã—ãŸï¼")
        return 0
    else:
        print(f"\nâš ï¸ {failed}ä»¶ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
