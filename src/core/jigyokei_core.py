"""
Jigyokei Core Module
Main AI Interviewer class for BCP (Business Continuity Plan) generation.
Migrated to google-genai SDK (2026-01-07)
"""
import os
import json
import re
import tempfile
from google import genai
import streamlit as st


class AIInterviewer:
    """
    Gemini 2.5 Flash ã‚’ä½¿ç”¨ã—ãŸãƒãƒ£ãƒƒãƒˆç®¡ç†ã‚¯ãƒ©ã‚¹ã€‚
    å±¥æ­´ã®ä¿æŒã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é©ç”¨ã‚’è¡Œã†ã€‚
    (Updated for Phase 3: Analysis Features)
    (Migrated to google-genai SDK)
    """
    def __init__(self):
        self.history = []
        self.uploaded_file_refs = []  # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®å‚ç…§ä¿æŒç”¨
        self.focus_fields = []  # AIãŒé‡ç‚¹çš„ã«èãã¹ãä¸è¶³é …ç›®ã®ãƒªã‚¹ãƒˆ
        self.client = None  # google-genai Client
        self.model_name = 'gemini-2.5-flash'
        self.chat_history = []  # For multi-turn chat (google-genai format)
        
        # åŸºæœ¬ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (Updated via NotebookLM)
        self.base_system_prompt = """
# Role Definition
ã‚ãªãŸã¯ã€ä¸­å°ä¼æ¥­ã®ã€Œäº‹æ¥­ç¶™ç¶šåŠ›å¼·åŒ–è¨ˆç”»ã€ç­–å®šæ”¯æ´ã‚’è¡Œã†ã€è¦ªåˆ‡ã§å„ªç§€ãªAIã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ã‚ãªãŸã®ç›®çš„ã¯ã€å¯¾è©±ã‚’é€šã˜ã¦é›»å­ç”³è«‹ã«å¿…è¦ãªæƒ…å ±ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰å¼•ãå‡ºã—ã€ç”³è«‹ã‚·ã‚¹ãƒ†ãƒ ã¸ã®å…¥åŠ›ãŒå¯èƒ½ãªå½¢å¼ï¼ˆæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼‰ã«æ•´ç†ã™ã‚‹ã“ã¨ã§ã™ã€‚

# Tone & Style
- **è¦ªã—ã¿ã‚„ã™ã•**: ä¸å¯§èªï¼ˆã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ï¼‰ã‚’ä½¿ç”¨ã—ã€æ¸©ã‹ã¿ã®ã‚ã‚‹ã‚³ãƒ³ã‚·ã‚§ãƒ«ã‚¸ãƒ¥ã®ã‚ˆã†ã«æ¥ã—ã¦ãã ã•ã„ã€‚
- **å°‚é–€æ€§**: å°‚é–€ç”¨èªã¯ãªã‚‹ã¹ãå™›ã¿ç •ãã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå›ç­”ã«è©°ã¾ã£ãŸéš›ã¯ã€æ¥­ç¨®åˆ¥ã®å…·ä½“ä¾‹ï¼ˆå»ºè¨­æ¥­ã€è£½é€ æ¥­ã€å°å£²æ¥­ãªã©ï¼‰ã‚’æç¤ºã—ã¦å°ã„ã¦ãã ã•ã„ã€‚
- **åŠ±ã¾ã—**: è¨ˆç”»ç­–å®šãŒä¼æ¥­ã®ä¿¡é ¼æ€§å‘ä¸Šã‚„å¼·é­åŒ–ã«ã¤ãªãŒã‚‹ã“ã¨ã‚’æ„è­˜ã•ã›ã€å‰å‘ããªå¯¾è©±ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚

# Operational Constraints (é‡è¦)
1. **ä¸€å•ä¸€ç­”ã®åŸå‰‡**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®è³ªå•ã¯**ä¸€åº¦ã«1ã¤**ã«é™å®šã—ã¦ãã ã•ã„ã€‚è¤‡æ•°ã®è³ªå•ã‚’ã¾ã¨ã‚ã¦æŠ•ã’ã‹ã‘ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
2. **é †æ¬¡é€²è¡Œ**: å¾Œè¿°ã™ã‚‹ã€Interaction Flowã€‘ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³é †ï¼ˆStep 1 â†’ Step 8ï¼‰ã«å¾“ã£ã¦é€²è¡Œã—ã¦ãã ã•ã„ã€‚
3. **ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå…¥åŠ›æ¤œè¨¼ï¼‰**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å›ç­”ãŒã€Input Rulesã€‘ã«é•åã—ã¦ã„ã‚‹å ´åˆã€å„ªã—ãæŒ‡æ‘˜ã—ã€æ­£ã—ã„å½¢å¼ã§ã®å†å…¥åŠ›ã‚’ä¿ƒã—ã¦ãã ã•ã„ã€‚
4. **ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³é€£æº**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œä½•ã‚’ã‹ã‘ã°ã„ã„ã‹åˆ†ã‹ã‚‰ãªã„ã€ç­‰ã®åå¿œã‚’ç¤ºã—ãŸå ´åˆã€ã€Reference Examplesã€‘ã®å†…å®¹ã‚’å‚ç…§ã—ã¦åŠ©è¨€ã—ã¦ãã ã•ã„ã€‚

# Input Rules (é›»å­ç”³è«‹ã‚·ã‚¹ãƒ†ãƒ ã®åˆ¶ç´„)
- **æ•°å€¤**: å…¨ã¦ã€ŒåŠè§’æ•°å­—ã€ã§å…¥åŠ›ã•ã›ã‚‹ã“ã¨ã€‚ï¼ˆä¾‹: 1000ï¼‰
- **ä»£è¡¨è€…æ°å**: å§“ã¨åã®é–“ã«å¿…ãšã€Œå…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ã€ã‚’1ã¤å…¥ã‚Œã‚‹ã“ã¨ã€‚ï¼ˆä¾‹: çµŒæ¸ˆã€€å¤ªéƒï¼‰
- **äº‹æ¥­æ‰€ä½æ‰€**: ç™»è¨˜ä¸Šã®æ­£ç¢ºãªä½æ‰€ã‚’å…¥åŠ›ã•ã›ã‚‹ã“ã¨ã€‚
- **å¿…é ˆè¦ä»¶**: ã€Œè‡ªç„¶ç½å®³ï¼ˆåœ°éœ‡ã€æ°´å®³ç­‰ï¼‰ã€ã®æƒ³å®šã¯å¿…é ˆã€‚æ„ŸæŸ“ç—‡ã‚„ã‚µã‚¤ãƒãƒ¼æ”»æ’ƒã¯æ¨å¥¨ã ãŒä»»æ„ã€‚

# Output Rules (Response Format)
- **æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å›ç­”ã®æœ€å¾Œï¼ˆæœ«å°¾ï¼‰ã«ã€å¿…ãšä»¥ä¸‹ã®XMLã‚¿ã‚°ã§å›²ã‚“ã JSONãƒ‡ãƒ¼ã‚¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
  ```xml
  <suggestions>
  {
    "options": ["ã¯ã„", "ã„ã„ãˆ", "é¸æŠè‚¢A", "é¸æŠè‚¢B"], 
    "hints": "å›ç­”ã®ãƒ’ãƒ³ãƒˆï¼ˆä¾‹ï¼šè£½é€ æ¥­ã®å ´åˆã¯...ï¼‰",
    "example": "å›ç­”ä¾‹ï¼ˆä¾‹ï¼šå·¥å ´å†…ã®é‡è¦è¨­å‚™ã¨ã—ã¦ã€Xå·æ©Ÿãƒ—ãƒ¬ã‚¹æ©ŸãŒã‚ã‚Šã¾ã™ã€‚ï¼‰"
  }
  </suggestions>
  ```
- **options**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãƒ¯ãƒ³ã‚¿ãƒƒãƒ—ã§è¿”ä¿¡ã§ãã‚‹çŸ­ã„é¸æŠè‚¢ï¼ˆæœ€å¤§4ã¤ï¼‰ã€‚ã€Œã¯ã„/ã„ã„ãˆã€ã‚„ã€å…·ä½“çš„ãªå€™è£œï¼ˆã€Œåœ°éœ‡ã€ã€Œæ°´å®³ã€ãªã©ï¼‰ã€‚
- **hints**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè€ƒãˆã‚„ã™ãã™ã‚‹ãŸã‚ã®è¦³ç‚¹ã‚„ã€æ¥­ç•Œåˆ¥ã®ä¸€èˆ¬çš„ãªå‚¾å‘ã€‚
- **example**: å…·ä½“çš„ãªå›ç­”ã®ä¾‹æ–‡ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã“ã‚Œã‚’å‚è€ƒã«æ–‡ç« ã‚’ä½œã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

---

# Interaction Flow

ä»¥ä¸‹ã®æ‰‹é †ã§ãƒ’ã‚¢ãƒªãƒ³ã‚°ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
**é‡è¦**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ—¢ã«é•·æ–‡ã‚„è³‡æ–™ã§è©³ç´°ãªæƒ…å ±ã‚’æä¾›ã—ã¦ã„ã‚‹å ´åˆã¯ã€æ©Ÿæ¢°çš„ã«è³ªå•ã‚’ç¹°ã‚Šè¿”ã•ãšã€ã€Œæƒ…å ±ã®ç¢ºèªï¼ˆVerificationï¼‰ã€ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã¦é€²è¡Œã—ã¦ãã ã•ã„ã€‚

## STEP 1: åŸºæœ¬æƒ…å ±ã®å®Œå…¨ãƒ’ã‚¢ãƒªãƒ³ã‚° (Basic Info Complete Interview)
1. **å¿…é ˆé …ç›®ã®ç¶²ç¾…çš„ç¢ºèª**: ä»¥ä¸‹ã®é …ç›®ã¯é›»å­ç”³è«‹ã§**å¿…é ˆ**ã§ã™ã€‚æœªå…¥åŠ›ã®é …ç›®ãŒãªã„ã‚ˆã†ã€é †æ¬¡ãƒ’ã‚¢ãƒªãƒ³ã‚°ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚ï¼ˆä¸€åº¦ã«å…¨ã¦èã‹ãšã€ä¸€ã¤ãšã¤ä¸å¯§ã«èã„ã¦ãã ã•ã„ï¼‰
   - **äº‹æ¥­è€…åï¼ˆæ­£å¼åç§°ï¼‰**: ã¾ãšã¯ã“ã‚Œã ã‘ã‚’èã„ã¦ãã ã•ã„ã€‚
   - **ãƒ•ãƒªã‚¬ãƒŠ**: äº‹æ¥­è€…åã®å›ç­”ãŒã‚ã£ãŸå¾Œã«ã€ã€Œãã®ãƒ•ãƒªã‚¬ãƒŠã‚’ã‚«ã‚¿ã‚«ãƒŠã§ã€ã¨èã„ã¦ãã ã•ã„ã€‚
   - **æ³•äººç•ªå·ï¼ˆ13æ¡ï¼‰**: åˆ†ã‹ã‚‰ãªã‘ã‚Œã°ã€Œå¾Œã§ç¢ºèªã€ã¨ã„ã†é¸æŠè‚¢ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚
   - **è¨­ç«‹å¹´æœˆæ—¥**: åˆ†ã‹ã‚‰ãªã‘ã‚Œã°ã€Œå¾Œã§ç¢ºèªã™ã‚‹ã€ã¨ã„ã†é¸æŠè‚¢ã‚’æç¤ºã—ã¦ãã ã•ã„ï¼ˆoptionsã«å«ã‚ã‚‹ï¼‰ã€‚
   - **ä½æ‰€è©³ç´°**: 
     - **éƒµä¾¿ç•ªå·**: ãƒã‚¤ãƒ•ãƒ³ã‚ã‚Šï¼ˆ641-0054ï¼‰ã§ã‚‚ãªã—ï¼ˆ6410054ï¼‰ã§ã‚‚**ãã®ã¾ã¾å—ã‘å…¥ã‚Œã¦ãã ã•ã„**ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å†å…¥åŠ›ã‚’æ±‚ã‚ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
     - **ä½æ‰€å…¥åŠ›**: éƒµä¾¿ç•ªå·ã‚’èã„ãŸç›´å¾Œã€ãã“ã‹ã‚‰æ¨æ¸¬ã•ã‚Œã‚‹ä½æ‰€ï¼ˆçœŒãƒ»å¸‚ãƒ»ç”ºåï¼‰ã‚’æç¤ºã—ã€ã€Œç•ªåœ°ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€ã¨ä¿ƒã—ã¦ãã ã•ã„ã€‚
     - **é‡è¦**: ã“ã®æ™‚ã€JSONã® `example` ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã¯ã€Œå’Œæ­Œå±±çœŒå’Œæ­Œå±±å¸‚ã€‡ã€‡ç”º1-1ã€ã®ã‚ˆã†ã«ã€**æ¨æ¸¬ã•ã‚ŒãŸä½æ‰€ï¼‹ä»®ã®ç•ªåœ°** ã‚’ã‚»ãƒƒãƒˆã—ã¦ãã ã•ã„ã€‚
   - **ä»£è¡¨è€…æƒ…å ±**: å½¹è·ã€æ°åï¼ˆå§“ã¨åã®é–“ã«å…¨è§’ã‚¹ãƒšãƒ¼ã‚¹ï¼‰ã€‚
   - **æ¥­ç¨®ï¼ˆå¤§åˆ†é¡ãƒ»ä¸­åˆ†é¡ï¼‰**: 
     - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã„ããªã‚Šã€Œä¸­åˆ†é¡ã€ã‚’èã„ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚
     - ã¾ãšã€Œã©ã®ã‚ˆã†ãªãŠä»•äº‹ã‚’ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿï¼ˆä¾‹ï¼šæ›¸é“æ•™å®¤ã€ãƒ©ãƒ¼ãƒ¡ãƒ³å±‹ï¼‰ã€ã¨**å…·ä½“çš„ãªäº‹æ¥­å†…å®¹**ã‚’èã„ã¦ãã ã•ã„ã€‚
     - äº‹æ¥­å†…å®¹ã‚’èãå–ã£ãŸã‚‰ã€ã‚ãªãŸãŒè²¬ä»»ã‚’æŒã£ã¦æ—¥æœ¬æ¨™æº–ç”£æ¥­åˆ†é¡ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚
   - **è³‡æœ¬é‡‘åˆã¯å‡ºè³‡ã®é¡**
   - **å¸¸æ™‚ä½¿ç”¨ã™ã‚‹å¾“æ¥­å“¡ã®æ•°**
2. **ç¢ºèª**: ã€Œå…¥åŠ›ã•ã‚ŒãŸåŸºæœ¬æƒ…å ±ï¼ˆä¸Šè¨˜ï¼‰ã«é–“é•ã„ãŒãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã€

## STEP 2: äº‹æ¥­å†…å®¹ã®æ·±æ˜ã‚Š (Deep Dive into Business)
**é‡è¦**: ã“ã“ã¯å½¢å¼çš„ãªç¢ºèªã§çµ‚ã‚ã‚‰ã›ãšã€è¨ˆç”»æ›¸ã®ä¸‹æ›¸ãã¨ã—ã¦ååˆ†ãªãƒœãƒªãƒ¥ãƒ¼ãƒ ï¼ˆæ–‡å­—æ•°ï¼‰ã‚’ç¢ºä¿ã§ãã‚‹ã‚ˆã†ã€1ã¤ãšã¤æ·±æ˜ã‚Šã—ã¦èã„ã¦ãã ã•ã„ã€‚

## STEP 3: ç½å®³ãƒªã‚¹ã‚¯ã®æƒ³å®š (Scenarios)
1. **ç½å®³ç¨®åˆ¥**: ã€Œå¾¡ç¤¾ã®äº‹æ¥­ã«æœ€ã‚‚å½±éŸ¿ã‚’ä¸ãˆã‚‹ã€è‡ªç„¶ç½å®³ã€ã‚’1ã¤é¸ã‚“ã§ãã ã•ã„ï¼ˆåœ°éœ‡ã€æ°´å®³ãªã©ï¼‰ã€‚ã€
2. **è¢«å®³æƒ³å®š**: ã€Œãã®ç½å®³ãŒèµ·ããŸæ™‚ã€ä»¥ä¸‹ã®4ã¤ã®è³‡æºã«ã©ã®ã‚ˆã†ãªè¢«å®³ãŒå‡ºã‚‹ã¨æƒ³å®šã•ã‚Œã¾ã™ã‹ï¼Ÿã€

## STEP 4: åˆå‹•å¯¾å¿œ (First Response)
1. **äººå‘½å®‰å…¨ç¢ºä¿**: ã€Œç™ºç½ç›´å¾Œã€å¾“æ¥­å“¡ã®é¿é›£ã‚„å®‰å¦ç¢ºèªã¯ã©ã®ã‚ˆã†ã«è¡Œã„ã¾ã™ã‹ï¼Ÿã€
2. **ç·Šæ€¥æ™‚ä½“åˆ¶**: ã€Œèª°ã‚’æœ¬éƒ¨é•·ã¨ã—ã¦ã€ã©ã®ã‚ˆã†ãªåŸºæº–ã§ç½å®³å¯¾ç­–æœ¬éƒ¨ã‚’ç«‹ã¡ä¸Šã’ã¾ã™ã‹ï¼Ÿã€
3. **è¢«å®³çŠ¶æ³æŠŠæ¡**: ã€Œè¢«å®³çŠ¶æ³ã‚’ã©ã®ã‚ˆã†ã«ç¢ºèªã—ã€èª°ï¼ˆå–å¼•å…ˆã€è‡ªæ²»ä½“ç­‰ï¼‰ã¸é€£çµ¡ã—ã¾ã™ã‹ï¼Ÿã€

## STEP 5: å¯¾ç­– (Measures)
â€»ã€Œç¾åœ¨ã§ãã¦ã„ã‚‹ã“ã¨ã€ã¨ã€Œä»Šå¾Œã®è¨ˆç”»ã€ã‚’ã‚»ãƒƒãƒˆã§èãã€‚

## STEP 6: æ¨é€²ä½“åˆ¶ (Implementation)
1. **å¹³æ™‚ã®ä½“åˆ¶**: ã€Œè¨ˆç”»ã‚’æ¨é€²ã™ã‚‹ãŸã‚ã«ã€çµŒå–¶å±¤ã¯ã©ã®ã‚ˆã†ã«é–¢ä¸ã—ã¾ã™ã‹ï¼Ÿã€
2. **è¨“ç·´ãƒ»è¦‹ç›´ã—**: ã€Œè¨“ç·´ã¨è¨ˆç”»ã®è¦‹ç›´ã—ã¯ã€ãã‚Œãã‚Œå¹´1å›ä»¥ä¸Šå®Ÿæ–½ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã€

## STEP 7: è³‡é‡‘è¨ˆç”»ãƒ»æœŸé–“ (Finance & Period)

## STEP 8: ãã®ä»–ãƒ»é€£çµ¡å…ˆ (Contact)

---

# Reference Examples (å›°ã£ãŸã¨ãã®åŠ©è¨€é›†)
"""

        # Extraction System Prompt for Agentic Mode
        self.extraction_system_prompt = """
        ã‚ãªãŸã¯ç†Ÿç·´ã®ä¸­å°ä¼æ¥­è¨ºæ–­å£«å…¼AIãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
        æä¾›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚„è³‡æ–™ã‹ã‚‰ã€äº‹æ¥­ç¶™ç¶šåŠ›å¼·åŒ–è¨ˆç”»ï¼ˆBCPï¼‰ã®ç”³è«‹ã«å¿…è¦ãªæƒ…å ±ã‚’æ¼ã‚‰ã•ãšã€æ­£ç¢ºã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
        
        ã€æŠ½å‡ºæ–¹é‡ã€‘
        - æ›–æ˜§ãªè¡¨ç¾ã¯é¿ã‘ã€äº‹å®Ÿã«åŸºã¥ã„ãŸæƒ…å ±ã®ã¿ã‚’æŠ½å‡ºã™ã‚‹ã“ã¨ã€‚
        - è©²å½“ã™ã‚‹æƒ…å ±ãŒãªã„å ´åˆã¯ã€ç„¡ç†ã«åŸ‹ã‚ãšã« null ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚
        - ç‰¹ã«ã€Œäº‹æ¥­ç¶™ç¶šåŠ›å¼·åŒ–ã®ç›®çš„ã€ã€Œç½å®³ãƒªã‚¹ã‚¯ã€ã€Œåˆå‹•å¯¾å¿œã€ã«é–¢ã™ã‚‹è¨˜è¿°ã‚’é‡ç‚¹çš„ã«æ¢ã™ã“ã¨ã€‚
        """

        # Streamlit Secrets ã¾ãŸã¯ ç’°å¢ƒå¤‰æ•°ã‹ã‚‰APIã‚­ãƒ¼ã‚’å–å¾—
        api_key = None
        try:
            api_key = st.secrets.get("GEMINI_API_KEY") or st.secrets.get("GOOGLE_API_KEY")
        except Exception:
            pass
        
        if not api_key:
            api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")

        if api_key:
            try:
                # google-genai Client initialization
                self.client = genai.Client(api_key=api_key)
            except Exception as e:
                st.error(f"Failed to initialize Gemini client: {e}")
                self.client = None
        else:
            self.client = None
            st.error("Google API Key not found. Please set GEMINI_API_KEY in Streamlit Secrets.")

    def set_focus_fields(self, fields: list):
        """
        AIãŒé‡ç‚¹çš„ã«èãã¹ãé …ç›®ã‚’è¨­å®šã™ã‚‹ã€‚
        ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã®è§£æçµæœã«åŸºã¥ã„ã¦å‘¼ã³å‡ºã•ã‚Œã‚‹ã“ã¨ã‚’æƒ³å®šã€‚
        """
        self.focus_fields = fields

    def process_files(self, uploaded_files, target_persona: str = None):
        """
        Streamlitã®UploadedFileãƒªã‚¹ãƒˆã‚’å—ã‘å–ã‚Šã€Gemini File APIã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€
        ãƒãƒ£ãƒƒãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ç™»éŒ²ã™ã‚‹ã€‚
        """
        if not self.client:
            return 0
            
        count = 0
        new_files = []
        
        for up_file in uploaded_files:
            # MIMEã‚¿ã‚¤ãƒ—ç°¡æ˜“åˆ¤å®š
            mime_type = up_file.type
            if not mime_type:
                # æ‹¡å¼µå­ã‹ã‚‰æ¨æ¸¬ï¼ˆæœ€ä½é™ï¼‰
                ext = up_file.name.split('.')[-1].lower()
                if ext in ['png', 'jpg', 'jpeg']: mime_type = 'image/jpeg'
                elif ext == 'pdf': mime_type = 'application/pdf'
                else: mime_type = 'application/pdf'  # Default

            try:
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
                with tempfile.NamedTemporaryFile(delete=False, suffix=f".{up_file.name.split('.')[-1]}") as tmp:
                    tmp.write(up_file.getvalue())
                    tmp_path = tmp.name
                
                # google-genai: Use client.files.upload
                g_file = self.client.files.upload(file=tmp_path, config={"mime_type": mime_type, "display_name": up_file.name})
                
                self.uploaded_file_refs.append(g_file)
                new_files.append(g_file)
                count += 1
                
                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                os.unlink(tmp_path)
                
            except Exception as e:
                print(f"File upload failed: {e}")
                st.error(f"Error uploading {up_file.name}: {e}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†ã®ãŠçŸ¥ã‚‰ã›ã‚’å±¥æ­´ã«è¿½åŠ 
        if new_files:
            self.history.append({
                "role": "model",
                "content": f"ğŸ“ {count}ä»¶ã®è³‡æ–™ã‚’å—ã‘å–ã‚Šã¾ã—ãŸã€‚\nå†…å®¹ã‚’ç¢ºèªã—ã¦ã€åˆ†ã‹ã‚‹éƒ¨åˆ†ã¯å…¥åŠ›ã‚’çœç•¥ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã­ã€‚",
                "persona": "AI Concierge",
                "target_persona": target_persona
            })

        return count

    def send_message(self, user_input: str, persona: str = "çµŒå–¶è€…", user_data: dict = None) -> str:
        if not self.client:
            return "Error: API Key is missing or model initialization failed."

        # å±¥æ­´ã¸ã®è¿½åŠ ï¼ˆã‚¢ãƒ—ãƒªè¡¨ç¤ºç”¨ï¼‰
        self.history.append({
            "role": "user", 
            "content": user_input,
            "persona": persona,
            "user_data": user_data
        })
        
        # å®Ÿéš›ã«APIã«é€ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        actual_prompt = user_input
        
        # Inject User Name context
        if user_data and (user_data.get("name") or user_data.get("position")):
            name = user_data.get("name", "")
            pos = user_data.get("position", "")
            display_str = f"{pos} {name}".strip()
            actual_prompt = f"ã€ç™ºè¨€è€…æƒ…å ±: {persona} ({display_str})ã€‘\n{user_input}"

        # Gap-Fillingã®ãŸã‚ã®èª˜å°ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä»˜ä¸ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯è¦‹ãˆãªã„ï¼‰
        if self.focus_fields:
            fields_str = ", ".join(self.focus_fields)
            actual_prompt += f"""
            
            [System Instruction for AI]
            ç¾åœ¨ã€ä»¥ä¸‹ã®é …ç›®ãŒã¾ã å…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“: {fields_str}
            
            ã€è¡Œå‹•æŒ‡é‡ï¼šä¼šè©±ã®æµã‚Œã‚’çµ¶å¯¾å„ªå…ˆã€‘
            1. **ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è©±é¡Œå„ªå…ˆ**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç‹¬è‡ªã®è©±é¡Œã‚’è©±ã—ã¦ã„ã‚‹å ´åˆã€ãã®è©±é¡Œã«å¾¹åº•çš„ã«å¯„ã‚Šæ·»ã£ã¦ãã ã•ã„ã€‚
            2. **è‡ªç„¶ãªç§»è¡Œ**: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è©±ãŒä¸€æ®µè½ã—ãŸã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ã®ã¿ã€æœªå…¥åŠ›é …ç›®ã«ã¤ã„ã¦åˆ‡ã‚Šå‡ºã—ã¦ãã ã•ã„ã€‚
            3. **å¼·å¼•ãªèª˜å°ã®ç¦æ­¢**: è„ˆçµ¡ãªãã€Œã¨ã“ã‚ã§ã€ã€‡ã€‡ã¯ï¼Ÿã€ã¨åˆ‡ã‚Šå‡ºã™ã“ã¨ã¯ç¦æ­¢ã§ã™ã€‚
            """
        
        try:
            # Build contents with chat history for multi-turn
            contents = []
            
            # Add system instruction
            contents.append({
                "role": "user",
                "parts": [{"text": self.base_system_prompt}]
            })
            contents.append({
                "role": "model", 
                "parts": [{"text": "äº†è§£ã—ã¾ã—ãŸã€‚äº‹æ¥­ç¶™ç¶šåŠ›å¼·åŒ–è¨ˆç”»ã®ç­–å®šæ”¯æ´ã‚’é–‹å§‹ã—ã¾ã™ã€‚"}]
            })
            
            # Add chat history
            for msg in self.chat_history:
                contents.append(msg)
            
            # Add current user message
            contents.append({
                "role": "user",
                "parts": [{"text": actual_prompt}]
            })
            
            # Send to Gemini using google-genai
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=contents
            )
            
            text_response = response.text
            
            # Post-processing to remove leaked thought process
            patterns = [
                r"^æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹:.*?(?:\n\n|\Z)",
                r"^Thinking Process:.*?(?:\n\n|\Z)",
                r"ã€æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã€‘.*?(?:\n\n|\Z)"
            ]
            for pat in patterns:
                text_response = re.sub(pat, "", text_response, flags=re.DOTALL | re.MULTILINE).strip()
            
            # Update chat history for multi-turn
            self.chat_history.append({
                "role": "user",
                "parts": [{"text": actual_prompt}]
            })
            self.chat_history.append({
                "role": "model",
                "parts": [{"text": text_response}]
            })
            
            self.history.append({
                "role": "model",
                "content": text_response,
                "persona": "AI Concierge",
                "target_persona": persona
            })
            return text_response
            
        except Exception as e:
            error_msg = f"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            self.history.append({
                "role": "model", 
                "content": error_msg,
                "persona": "AI Concierge",
                "target_persona": persona
            })
            return error_msg

    def analyze_history(self) -> dict:
        """
        ç¾åœ¨ã®ä¼šè©±å±¥æ­´ã‚’åˆ†æã—ã€JigyokeiPlanã‚¹ã‚­ãƒ¼ãƒã«é©åˆã™ã‚‹JSONã‚’ç”Ÿæˆã™ã‚‹ã€‚
        """
        if not self.history:
            return {}
        
        if not self.client:
            return {}

        # å±¥æ­´ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
        history_text = ""
        for msg in self.history:
            role = msg["role"]
            content = msg["content"]
            persona = msg.get("persona", "")
            history_text += f"[{role} ({persona})]: {content}\n"

        prompt = f"""
        ã‚ãªãŸã¯äº‹æ¥­ç¶™ç¶šåŠ›å¼·åŒ–è¨ˆç”»ã®ç­–å®šæ”¯æ´AIã§ã™ã€‚
        ä»¥ä¸‹ã®ä¼šè©±å±¥æ­´ã‹ã‚‰ã€äº‹æ¥­è¨ˆç”»æ›¸ã®ä½œæˆã«å¿…è¦ãªæƒ…å ±ã‚’æŠ½å‡ºã—ã€JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
        
        ã€æŠ½å‡ºãƒ«ãƒ¼ãƒ«ã€‘
        1. ä»¥ä¸‹ã®JSONã‚¹ã‚­ãƒ¼ãƒã«å¾“ã†ã“ã¨ã€‚
        2. æƒ…å ±ãŒãªã„é …ç›®ã¯ null ã¾ãŸã¯ "æœªè¨­å®š" ã¨ã™ã‚‹ã€‚
        3. æ¨æ¸¬ã¯ã›ãšã€ä¼šè©±ã«å‡ºã¦ããŸäº‹å®Ÿã®ã¿ã‚’æŠ½å‡ºã™ã‚‹ã“ã¨ã€‚

        ã€ä¼šè©±å±¥æ­´ã€‘
        {history_text}

        ã€å‡ºåŠ›ã‚¹ã‚­ãƒ¼ãƒã€‘
        {{
            "basic_info": {{
                "applicant_type": "æ³•äºº or å€‹äººäº‹æ¥­ä¸»",
                "corporate_name": "ä¼æ¥­å",
                "corporate_name_kana": "ãƒ•ãƒªã‚¬ãƒŠ",
                "representative_title": "å½¹è·",
                "representative_name": "ä»£è¡¨è€…å",
                "address_zip": "éƒµä¾¿ç•ªå·",
                "address_pref": "éƒ½é“åºœçœŒ",
                "address_city": "å¸‚åŒºç”ºæ‘",
                "address_street": "ç•ªåœ°",
                "address_building": "å»ºç‰©å",
                "capital": 0,
                "employees": 0,
                "establishment_date": "YYYY/MM/DD",
                "industry_major": "å¤§åˆ†é¡",
                "industry_middle": "ä¸­åˆ†é¡",
                "industry_minor": "å°åˆ†é¡",
                "corporate_number": "æ³•äººç•ªå·"
            }},
            "goals": {{
                "business_overview": "äº‹æ¥­æ¦‚è¦",
                "business_purpose": "ç›®çš„",
                "disaster_scenario": {{
                    "disaster_assumption": "æƒ³å®šç½å®³",
                    "impacts": {{
                        "impact_personnel": "äººå“¡ã¸ã®å½±éŸ¿",
                        "impact_building": "å»ºç‰©ã¸ã®å½±éŸ¿",
                        "impact_funds": "è³‡é‡‘ã¸ã®å½±éŸ¿",
                        "impact_info": "æƒ…å ±ã¸ã®å½±éŸ¿"
                    }}
                }}
            }},
            "response_procedures": [
                {{ "category": "...", "action_content": "...", "timing": "ç™ºç½ç›´å¾Œ", "preparation_content": "..." }}
            ],
            "measures": {{
                "personnel": {{ "current_measure": "...", "future_plan": "..." }},
                "building": {{ "current_measure": "...", "future_plan": "..." }},
                "money": {{ "current_measure": "...", "future_plan": "..." }},
                "data": {{ "current_measure": "...", "future_plan": "..." }}
            }},
            "pdca": {{
                "management_system": "...",
                "training_education": "...",
                "plan_review": "..."
            }}
        }}
        """

        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt
            )
            
            text = response.text
            
            # 1. Try finding Markdown Code Block
            json_match = re.search(r'```json\s*(.*?)\s*```', text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
            
            # 2. Try finding raw JSON structure
            raw_match = re.search(r'(\{.*\})', text, re.DOTALL)
            if raw_match:
                return json.loads(raw_match.group(1))
                
            # 3. Last resort
            return json.loads(text)
            
        except Exception as e:
            print(f"Analysis failed: {e}")
            return {}

    def detect_conflicts(self) -> dict:
        """
        å…¨ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’åˆ†æã—ã€ãƒšãƒ«ã‚½ãƒŠé–“ã®æ„è¦‹ã®ä¸ä¸€è‡´ã‚„çŸ›ç›¾ã‚’æŠ½å‡ºã™ã‚‹ã€‚
        """
        if not self.history:
            return {"conflicts": []}
        
        if not self.client:
            return {"conflicts": []}

        # å±¥æ­´ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
        history_text = ""
        for msg in self.history:
            role = msg["role"]
            content = msg["content"]
            persona = msg.get("persona", "Unknown")
            user_data = msg.get("user_data", {})
            
            sender_info = persona
            if user_data:
                name = user_data.get("name")
                pos = user_data.get("position")
                if name: sender_info += f" ({name})"
                if pos: sender_info += f" [{pos}]"
            
            history_text += f"[{sender_info}]: {content}\n"

        prompt = f"""
        ã‚ãªãŸã¯äº‹æ¥­ç¶™ç¶šåŠ›å¼·åŒ–è¨ˆç”»ç­–å®šã®ã€ŒçŸ›ç›¾æ¤œçŸ¥ãƒ»åˆæ„å½¢æˆæ”¯æ´AIã€ã§ã™ã€‚
        ä»¥ä¸‹ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’åˆ†æã—ã€ãƒšãƒ«ã‚½ãƒŠé–“ã§ã€Œäº‹å®Ÿèªè­˜ã€ã‚„ã€Œæ„è¦‹ã€ã«é£Ÿã„é•ã„ãŒã‚ã‚‹ç‚¹ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

        ã€å›ç­”å½¢å¼ã€‘
        ä»¥ä¸‹ã®JSONå½¢å¼ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚çŸ›ç›¾ãŒãªã„å ´åˆã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
        
        {{
            "conflicts": [
                {{
                    "topic": "çŸ›ç›¾ã—ã¦ã„ã‚‹ãƒˆãƒ”ãƒƒã‚¯",
                    "persona_A": "çµŒå–¶è€…",
                    "statement_A": "ç™ºè¨€å†…å®¹A",
                    "persona_B": "å¾“æ¥­å“¡",
                    "statement_B": "ç™ºè¨€å†…å®¹B",
                    "suggestion": "è§£æ±ºææ¡ˆ"
                }}
            ]
        }}

        ã€ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã€‘
        {history_text}
        """

        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config={"response_mime_type": "application/json"}
            )
            return json.loads(response.text)
        except Exception as e:
            return {"conflicts": []}

    def merge_history(self, new_history: list):
        """
        æ–°ã—ã„å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’ç¾åœ¨ã®å±¥æ­´ã«çµ±åˆï¼ˆãƒãƒ¼ã‚¸ï¼‰ã™ã‚‹ã€‚
        """
        self.history.extend(new_history)
        self._rebuild_chat_history()

    def _rebuild_chat_history(self):
        """
        ç¾åœ¨ã® self.history ã«åŸºã¥ã„ã¦ chat_history ã‚’å†æ§‹ç¯‰ã™ã‚‹
        """
        self.chat_history = []
        for msg in self.history:
            role = "user" if msg["role"] == "user" else "model"
            self.chat_history.append({
                "role": role,
                "parts": [{"text": msg["content"]}]
            })

    def load_history(self, history_data: list, merge: bool = False):
        """
        å¤–éƒ¨ã‹ã‚‰å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ã€‚
        merge=Trueã®å ´åˆã€æ—¢å­˜ã®å±¥æ­´ã‚’ä¿æŒã—ãŸã¾ã¾è¿½åŠ ã™ã‚‹ï¼ˆãƒã‚¹ã‚¿ãƒ¼ãƒãƒ£ãƒƒãƒˆåŒ–ï¼‰ã€‚
        """
        if merge:
            self.merge_history(history_data)
        else:
            self.history = history_data
            self._rebuild_chat_history()

    def extract_structured_data(self, text: str = "", file_refs: list = None) -> dict:
        """
        Agentic Extraction:
        å…¥åŠ›ã•ã‚ŒãŸé•·æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚„è³‡æ–™ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬æŠ½å‡ºã™ã‚‹ã€‚
        """
        if not self.client:
            return {}
            
        try:
            content_parts = [self.extraction_system_prompt]
            
            if text:
                content_parts.append(f"\n\n# Input Text\n{text}")
            
            if file_refs:
                content_parts.append("\n\n# Input Documents (Already Uploaded)")
                # Note: File handling may need adjustment for new SDK
            
            content_parts.append("\n\n# Output JSON (Strict Schema Match ApplicationRoot)")
            
            full_prompt = "\n".join(content_parts)
            
            response = self.client.models.generate_content(
                model="gemini-1.5-pro",
                contents=full_prompt
            )
            
            # Extract JSON from code block
            match = re.search(r'```json\n(.*?)\n```', response.text, flags=re.DOTALL)
            if match:
                return json.loads(match.group(1))
            else:
                clean_text = response.text.strip()
                if clean_text.startswith("```json"):
                    clean_text = clean_text[7:]
                if clean_text.endswith("```"):
                    clean_text = clean_text[:-3]
                return json.loads(clean_text)
                 
        except Exception as e:
            print(f"Extraction Error: {e}")
            return {}
    
    # Compatibility property for old code that checks self.model
    @property
    def model(self):
        """Compatibility property - returns True if client is initialized."""
        return self.client is not None
