import sys
import os
import streamlit as st
import json
import time
import importlib
import streamlit.components.v1 as components

# --- Page Config (Must be the first Streamlit command) ---
st.set_page_config(
    page_title="Jigyokei Hybrid System",
    page_icon="ğŸ‘‘",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Path Setup ---
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

# --- Module Reloading for Streamlit Cloud ---
import src.core.jigyokei_core
import src.api.schemas
import src.core.completion_checker
import src.core.draft_exporter
importlib.reload(src.core.jigyokei_core)
importlib.reload(src.api.schemas)
importlib.reload(src.core.completion_checker)
importlib.reload(src.core.draft_exporter)

importlib.reload(src.core.draft_exporter)

from src.core.jigyokei_core import AIInterviewer
from src.data.context_loader import ContextLoader
from src.core.completion_checker import CompletionChecker
#         
#         # Restore History
#         history = saved_data["history"]
#         st.session_state.ai_interviewer.load_history(history, merge=False)
#         st.session_state.loaded_msg_count = len(history)
#         
#         # Restore Plan if exists
#         current_plan_dict = saved_data.get("current_plan")
#         if current_plan_dict:
#              try:
#                 from src.api.schemas import ApplicationRoot
#                 plan = ApplicationRoot.model_validate(current_plan_dict)
#                 st.session_state.current_plan = plan
#              except Exception:
#                  pass # Ignore plan restore error

if "app_version" not in st.session_state or st.session_state.app_version != APP_VERSION:
    st.session_state.clear()
    st.session_state.app_version = APP_VERSION
    st.rerun()

# --- Debug / Reset Controls ---
with st.sidebar:
    with st.expander("ğŸ”§ System Menu", expanded=False):
        if st.button("ğŸ—‘ï¸ Reset All Data", key="btn_hard_reset", type="primary", help="è­¦å‘Š: ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦åˆæœŸåŒ–ã—ã¾ã™"):
            st.session_state.clear()
            st.session_state.session_manager.clear_session()
            st.rerun()

# --- Initialize Managers (Standard) ---
if "ai_interviewer" not in st.session_state:
    st.session_state.ai_interviewer = AIInterviewer()
else:
    # Check for outdated instance (missing 'analyze_history')
    if not hasattr(st.session_state.ai_interviewer, "analyze_history"):
        st.warning("ğŸ”„ Upgrading AI Brain to latest version...")
        
        # Preserve old history
        old_history = getattr(st.session_state.ai_interviewer, "history", [])
        
        # Re-initialize with new class definition
        st.session_state.ai_interviewer = AIInterviewer()
        
        # Restore history logic... (simplified for this block, as load_history handles it)
        st.session_state.ai_interviewer.load_history(old_history)
        if hasattr(st.session_state.ai_interviewer, "load_history"):
             st.session_state.ai_interviewer.load_history(old_history)
        else:
             st.session_state.ai_interviewer.history = old_history
             
        st.success("âœ… AI Brain Upgraded! Please reload one last time.")
        time.sleep(1)
        st.rerun()

if "context_loader" not in st.session_state:
    root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
    context_dir = os.path.join(root_dir, "data", "context")
    st.session_state.context_loader = ContextLoader(context_dir)

# --- Authentication ---
def check_password():
    if st.session_state.get("password_correct", False):
        return True

    def password_entered():
        if st.session_state["password"] == st.secrets["APP_PASSWORD"]:
            st.session_state["password_correct"] = True
            del st.session_state["password"]
        else:
            st.session_state["password_correct"] = False

    st.text_input("Password", type="password", on_change=password_entered, key="password")
    return False

if not check_password():
    st.stop()

# ==========================================
# Main App Logic
# ==========================================

# State Transition Helper
def change_mode(mode_name, persona_name=None):
    # Map legacy args to new nav selection
    target = "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼" # Default
    
    if mode_name == "Chat Mode (Interview)":
        if persona_name == "çµŒå–¶è€…": target = "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼"
        elif persona_name == "å¾“æ¥­å“¡": target = "å¾“æ¥­å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼"
        elif persona_name == "å•†å·¥ä¼šè·å“¡": target = "å•†å·¥ä¼šè·å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼"
    elif mode_name == "Main Consensus Room (Resolution)":
        target = "Main Consensus Room (å…¨ä½“åˆæ„)"
    elif mode_name == "Dashboard Mode (Progress)":
        target = "Dashboard Mode (Progress)"
         
    st.session_state.app_nav_selection = target

with st.sidebar:
    st.header("Jigyokei Hybrid System")
    st.caption("Cloud Edition â˜ï¸")
    st.text(f"Ver: {APP_VERSION}") # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å¸¸ã«è¡¨ç¤º

    # --- Live Progress Indicator ---
    from src.core.completion_checker import CompletionChecker
    
    current_plan_obj = st.session_state.get("current_plan")
    if current_plan_obj:
        try:
             checker = CompletionChecker(current_plan_obj)
             # Basic Info is Step 1, Goals Step 2... Let's use overall completeness
             missing_count = len(checker.check_missing_fields())
             total_fields = 20 # Estimate
             progress = max(0, min(100, int((20 - missing_count) / 20 * 100)))
             
             st.divider()
             st.progress(progress / 100)
             st.caption(f"ç¾åœ¨ã®é€²æ—: {progress}% (æ®‹ã‚Šé …ç›®: {missing_count})")
             
             if st.button("ğŸ“Š é€²æ—è©³ç´°ã‚’ç¢ºèª (Dashboard)", key="sidebar_progress_btn"):
                 st.session_state.app_nav_selection = "Dashboard Mode (Progress)"
                 st.rerun()
             st.divider()
        except:
             pass
    
    # Navigation Selection
    if "app_nav_selection" not in st.session_state:
        st.session_state.app_nav_selection = "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼"

    # Determine current index for radio
    interview_options = ["çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", "å¾“æ¥­å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", "å•†å·¥ä¼šè·å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼"]
    current_nav = st.session_state.app_nav_selection
    
    radio_index = 0
    if current_nav in interview_options:
        radio_index = interview_options.index(current_nav)
        
    # Callback to update state from radio
    def on_radio_change():
        st.session_state.app_nav_selection = st.session_state.nav_radio_key

    selected_interview = st.radio(
        "ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼é¸æŠ",
        interview_options,
        index=radio_index,
        key="nav_radio_key",
        on_change=on_radio_change
    )
    
    # Logic derivation (Internal)
    nav = st.session_state.app_nav_selection
    
    # --- State Tracking for Navigation Flux (Dashboard Return) ---
    if "last_chat_nav" not in st.session_state:
        st.session_state.last_chat_nav = "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼"

    valid_return_targets = [
        "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", 
        "å¾“æ¥­å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", 
        "å•†å·¥ä¼šè·å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", 
        "Main Consensus Room (å…¨ä½“åˆæ„)"
    ]
    if nav in valid_return_targets:
        st.session_state.last_chat_nav = nav
    # -------------------------------------------------------------

    if nav == "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼":
        mode = "Chat Mode (Interview)"
        persona = "çµŒå–¶è€…"
    elif nav == "å¾“æ¥­å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼":
        mode = "Chat Mode (Interview)"
        persona = "å¾“æ¥­å“¡"
    elif nav == "å•†å·¥ä¼šè·å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼":
        mode = "Chat Mode (Interview)"
        persona = "å•†å·¥ä¼šè·å“¡"
    elif nav == "Main Consensus Room (å…¨ä½“åˆæ„)":
        mode = "Main Consensus Room (Resolution)"
        persona = "ç·åˆèª¿æ•´å½¹"
    elif nav == "Dashboard Mode (Progress)":
        mode = "Dashboard Mode (Progress)"
        persona = "Viewer"
    else: # Fallback
        mode = "Chat Mode (Interview)"
        persona = "çµŒå–¶è€…"

    # Manager Menu (Hidden by default)
    with st.expander("ç®¡ç†è€…ãƒ¡ãƒ‹ãƒ¥ãƒ¼", expanded=False):

        if st.button("å…¨ä½“åˆæ„ãƒ«ãƒ¼ãƒ  (Consensus)", use_container_width=True):
             st.session_state.app_nav_selection = "Main Consensus Room (å…¨ä½“åˆæ„)"
             st.rerun()
             
        if st.button("ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ (Progress)", use_container_width=True):
             st.session_state.app_nav_selection = "Dashboard Mode (Progress)"
             st.rerun()

        st.divider()
        st.caption("Data Management")
        
        # --- Upload (Import) ---
        import_owner_label = "ãƒ‡ãƒ¼ã‚¿æ‰€æœ‰è€… (ã‚¿ã‚°è£œå®Œç”¨)"
        import_owner = st.selectbox(
            import_owner_label, 
            ["è‡ªå‹• (Auto)", "çµŒå–¶è€…", "å¾“æ¥­å“¡", "å•†å·¥ä¼šè·å“¡"], 
            index=0,
            key="import_owner_select", # Changed key to avoid duplicate error if previously rendered? No, component moved.
            help="å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€éš›ã€èª°ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‹æŒ‡å®šã—ã¾ã™ã€‚ã€Œè‡ªå‹•ã€ã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®æƒ…å ±ã‚’å„ªå…ˆã—ã¾ã™ã€‚"
        )
        
        uploaded_file = st.file_uploader("JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—", type=["json"])
        
        if uploaded_file:
            file_id = f"{uploaded_file.name}_{uploaded_file.size}"
            if st.session_state.get("last_loaded_file_id") != file_id:
                try:
                    uploaded_file.seek(0)
                    data = json.load(uploaded_file)
                    
                    # Handle if data is a list (e.g. raw history list or basic_scenario)
                    if isinstance(data, list):
                        # Check if it looks like a valid history list (items must be dicts with 'role' and 'content')
                        if all(isinstance(m, dict) and "role" in m and "content" in m for m in data):
                             valid_history = data
                             if import_owner != "è‡ªå‹• (Auto)":
                                for msg in valid_history:
                                    if "persona" not in msg: msg["persona"] = import_owner
                                    if "target_persona" not in msg and msg.get("role") == "model": msg["target_persona"] = import_owner
                             
                             st.session_state.ai_interviewer.load_history(valid_history, merge=True)
                             st.session_state.loaded_msg_count = len(st.session_state.ai_interviewer.history)
                             st.toast(f"âœ… ä¼šè©±å±¥æ­´(ãƒªã‚¹ãƒˆå½¢å¼)ã‚’çµ±åˆã—ã¾ã—ãŸ ({len(valid_history)}ä»¶)", icon="ğŸ“¥")
                        else:
                             st.warning("âš ï¸ èª­ã¿è¾¼ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ (å¯¾å¿œå½¢å¼: äº‹æ¥­è¨ˆç”»JSON, ã¾ãŸã¯ä¼šè©±å±¥æ­´ãƒªã‚¹ãƒˆ)")
                    
                    elif isinstance(data, dict):
                        # 1. History Loading Logic (Existing Wrapper)
                        if "history" in data:
                            valid_history = [m for m in data.get("history", []) if isinstance(m, dict)]
                            
                            if import_owner != "è‡ªå‹• (Auto)":
                                for msg in valid_history:
                                    if "persona" not in msg and msg.get("role") == "user":
                                        msg["persona"] = import_owner
                                    if "target_persona" not in msg and msg.get("role") == "model":
                                        msg["target_persona"] = import_owner
                            
                            st.session_state.ai_interviewer.load_history(valid_history, merge=True)
                            st.session_state.loaded_msg_count = len(st.session_state.ai_interviewer.history)
                            st.toast(f"âœ… ä¼šè©±å±¥æ­´ã‚’çµ±åˆã—ã¾ã—ãŸ ({len(valid_history)}ä»¶)", icon="ğŸ“¥")

                        # 2. Direct Plan Loading Logic (New for Test Data)
                        # Check if 'basic_info' or 'goals' (key fields of ApplicationRoot) exists
                        elif "basic_info" in data or "goals" in data:
                            from src.api.schemas import ApplicationRoot
                            try:
                                # Migration Step
                                clean_data = ApplicationRoot.migrate_legacy_data(data)
                                
                                # Attempt to validate and load as current plan
                                plan = ApplicationRoot.model_validate(clean_data)
                                st.session_state.current_plan = plan
                                st.toast("âœ… äº‹æ¥­è¨ˆç”»ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ (Direct Load)", icon="ğŸ“„")
                            except Exception as val_e:
                                st.error(f"ãƒ‡ãƒ¼ã‚¿æ§‹é€ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {val_e}")
                                # Stop execution so user sees the error
                                st.stop()


                        
                        else:
                            st.warning("âš ï¸ èª­ã¿è¾¼ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ (history, basic_info, goals ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)")
                    else:
                         st.warning("âš ï¸ JSONå½¢å¼ãŒç„¡åŠ¹ã§ã™")

                    st.session_state.last_loaded_file_id = file_id
                    time.sleep(1)
                    # Only rerun if successful (toast would persist?) - actually Streamlit recommends rerun on state change
                    st.rerun()

                except Exception as e:
                    st.error(f"Error loading JSON: {e}")

        # --- Download (Export) ---
        if st.session_state.ai_interviewer.history:
            # 1. Full Backup
            full_history_json = json.dumps({"history": st.session_state.ai_interviewer.history}, indent=2, ensure_ascii=False)
            st.download_button(
                label="ğŸ“¦ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ (Backup All)",
                data=full_history_json,
                file_name=f"jigyokei_full_backup_{int(time.time())}.json",
                mime="application/json"
            )

            # 1.5 Draft Plan Export (Markdown) - Only if analyzed
            # TEMPORARY: Disabled Markdown Export due to Schema Migration (JigyokeiPlan -> ApplicationRoot)
            if False and "current_plan" in st.session_state and st.session_state.current_plan:
                plan_export = st.session_state.current_plan
                # Simple MD generation
                md_text = f"# äº‹æ¥­ç¶™ç¶šåŠ›å¼·åŒ–è¨ˆç”»ï¼ˆä¸‹æ›¸ãï¼‰\\n\\n"
                md_text += f"## åŸºæœ¬æƒ…å ±\\n- ä¼æ¥­å: {plan_export.basic_info.company_name}\\n- ä»£è¡¨è€…: {plan_export.basic_info.representative_name}\\n- ä½æ‰€: {plan_export.basic_info.address}\\n\\n"
                md_text += f"## äº‹æ¥­å†…å®¹\\n- é¡§å®¢: {plan_export.business_content.target_customers}\\n- å•†å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹: {plan_export.business_content.products_services}\\n- æä¾›æ–¹æ³•: {plan_export.business_content.delivery_methods}\\n- å¼·ã¿: {plan_export.business_content.core_competence}\\n\\n"
                md_text += f"## è¢«å®³æƒ³å®š (ãƒªã‚¹ã‚¯)\\n"
                for r in plan_export.disaster_risks:
                    md_text += f"- {r.risk_type}: {r.impact_description}\\n"
                md_text += f"\\n## äº‹å‰å¯¾ç­–\\n"
                for m in plan_export.pre_disaster_measures:
                    md_text += f"- {m.item}: {m.content} (æ‹…å½“: {m.in_charge})\\n"
                
                st.download_button(
                    label="ğŸ“ ä¸‹æ›¸ãã‚·ãƒ¼ãƒˆã‚’ä¿å­˜ (Markdown)",
                    data=md_text,
                    file_name=f"jigyokei_draft_{int(time.time())}.md",
                    mime="text/markdown",
                    help="è§£ææ¸ˆã¿ã®è¨ˆç”»æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚"
                )
            
            # 2. Persona Specific Export
            # Note: Need to access 'persona' variable which is derived LATER.
            # CRITICAL: We cannot access 'persona' here because it is defined AFTER this block.
            # Implication: We should calculate 'persona' inside the expander OR rely on session state if available.
            # BUT 'persona' depends on 'nav' which IS available (st.session_state.app_nav_selection).
            # Let's verify 'nav' logic.
             
             

# --- Main Area ---


if mode == "Chat Mode (Interview)":
    # 1. Dashboard Navigation & Header
    col_head1, col_head2 = st.columns([3, 1])
    with col_head1:
        st.title("ğŸ¤– AI Interviewer (Chat Mode)")
    with col_head2:
        st.button(
            "ğŸ“Š é€²æ—åº¦ã‚’ç¢ºèªã™ã‚‹",
            on_click=change_mode,
            args=("Dashboard Mode (Progress)",)
        )

    # User Metadata Inputs (Main Panel) - Always visible at top
    with st.container(border=True):
        st.caption(f"ğŸ“ {persona}æƒ…å ±å…¥åŠ›")
        col_u1, col_u2 = st.columns(2)
        with col_u1:
             pos_placeholder = "ä¾‹: ä»£è¡¨å–ç· å½¹"
             if persona == "å¾“æ¥­å“¡": pos_placeholder = "ä¾‹: ç¾å ´ç›£ç£"
             elif persona == "å•†å·¥ä¼šè·å“¡": pos_placeholder = "ä¾‹: çµŒå–¶æŒ‡å°å“¡"
             st.text_input("å½¹è· (Position)", key="user_position_input", placeholder=pos_placeholder)
        with col_u2:
             st.text_input("ãŠåå‰ (Name)", key="user_name_input", placeholder="ä¾‹: å±±ç”° å¤ªéƒ")

    # 2. Document Upload Area (Always Available)
    with st.expander("ğŸ“‚ è³‡æ–™ã®è¿½åŠ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (Upload Documents)", expanded=not st.session_state.ai_interviewer.history):
        # Persona-specific Guidance
        upload_label = "è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF/ç”»åƒ)"
        if persona == "çµŒå–¶è€…":
            st.info("ğŸ¢ **çµŒå–¶è€…ã®æ–¹ã¸**: ä¼šç¤¾æ¡ˆå†…ã€äº‹æ¥­è¨ˆç”»æ›¸ã€ãƒã‚¶ãƒ¼ãƒ‰ãƒãƒƒãƒ—ãªã©")
            upload_label = "ğŸ¢ çµŒå–¶è€…ç”¨è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
        elif persona == "å¾“æ¥­å“¡":
            st.info("ğŸ‘· **å¾“æ¥­å“¡ã®æ–¹ã¸**: æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã€ç·Šæ€¥é€£çµ¡ç¶²ã€ç¾å ´å†™çœŸãªã©")
            upload_label = "ğŸ‘· ç¾å ´ãƒ»æ¥­å‹™è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
        elif persona == "å•†å·¥ä¼šè·å“¡":
            st.info("ğŸ§‘â€ğŸ« **å•†å·¥ä¼šè·å“¡ã®æ–¹ã¸**: å…±æ¸ˆãƒ‘ãƒ³ãƒ•ãƒ¬ãƒƒãƒˆã€åœ°åŸŸé˜²ç½è¨ˆç”»ãªã©")
            upload_label = "ğŸ§‘â€ğŸ« æ”¯æ´ãƒ»åˆ¶åº¦è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
        
        uploaded_refs = st.file_uploader(
            upload_label, 
            type=["pdf", "png", "jpg", "jpeg"], 
            accept_multiple_files=True,
            key=f"uploader_{persona}" # Remove timestamp to keep uploader stable
        )
        
        # --- Auto-Process Logic ---
        if "processed_file_ids" not in st.session_state:
            st.session_state.processed_file_ids = set()

        if uploaded_refs:
            new_files_to_process = []
            for file in uploaded_refs:
                # Create a simple unique ID for the file instance
                file_id = f"{file.name}_{file.size}"
                if file_id not in st.session_state.processed_file_ids:
                    new_files_to_process.append(file)
                    st.session_state.processed_file_ids.add(file_id)
            
                if new_files_to_process:
                # Automatically process new files
                 with st.spinner("è³‡æ–™ã‚’è§£æä¸­... (Auto-Processing)"):
                    try:
                        count = st.session_state.ai_interviewer.process_files(new_files_to_process, target_persona=persona)
                        st.success(f"{count}ä»¶ã®æ–°ã—ã„è³‡æ–™ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼")
                        
                        # --- Agentic Extraction Trigger (File Upload) ---
                        if count > 0:
                            with st.status("ğŸ¤– AI Agent Working: è³‡æ–™ã‚’è©³ç´°åˆ†æä¸­...", expanded=True) as status:
                                 status.write("ğŸ“ Gemini 1.5 Pro (High Reasoning) ã§è³‡æ–™ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
                                 try:
                                     all_files = st.session_state.ai_interviewer.uploaded_file_refs
                                     extracted_data = st.session_state.ai_interviewer.extract_structured_data(text="", file_refs=all_files)
                                     
                                     if extracted_data:
                                         status.write("âœ… æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")
                                         status.write("ğŸ’¡ æŠ½å‡ºçµæœã¯ä¼šè©±ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«ä¿æŒã•ã‚Œã¾ã—ãŸã€‚")
                                     else:
                                         status.write("â„¹ï¸ æ–°è¦ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                                 except Exception as ex_e:
                                     status.error(f"Extraction Error: {ex_e}")
                        
                        time.sleep(1)
                        perform_auto_save()
                        st.rerun()
                    except Exception as e:
                        st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    # 3. Chat Interface
    st.divider()
    
    # History Display
    if not st.session_state.ai_interviewer.history:
        st.markdown(
            "ğŸ‘‹ **ã“ã‚“ã«ã¡ã¯ã€‚äº‹æ¥­ç¶™ç¶šåŠ›å¼·åŒ–è¨ˆç”»ã®ç­–å®šã‚’æ”¯æ´ã—ã¾ã™ã€‚**\n\n"
            "ã¾ãšã¯ä¸Šã®ã€Œè³‡æ–™ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã‹ã‚‰è³‡æ–™ã‚’èª­ã¿è¾¼ã¾ã›ã‚‹ã‹ã€"
            "ä¸‹ã®å…¥åŠ›æ¬„ã‹ã‚‰ä¼šè©±ã‚’å§‹ã‚ã¦ãã ã•ã„ã€‚"
        )
    
    # Helper to render a single message
    def render_message(msg, current_persona):
        if not isinstance(msg, dict): return
        role = msg["role"]
        msg_persona = msg.get("persona", "Unknown")
        target_persona = msg.get("target_persona")
        
        # Filtering Logic
        visible = False
        if role == "user" and msg_persona == current_persona:
            visible = True
        elif role == "model" and target_persona == current_persona:
            visible = True
        
        if visible:
            avatar = "ğŸ¤–" if role == "model" else "ğŸ‘¤"
            if msg_persona == "çµŒå–¶è€…": avatar = "ğŸ‘¨â€ğŸ’¼"
            elif msg_persona == "å¾“æ¥­å“¡": avatar = "ğŸ‘·"
            elif msg_persona == "å•†å·¥ä¼šè·å“¡": avatar = "ğŸ§‘â€ğŸ«"
            elif msg_persona == "AI Concierge": avatar = "ğŸ¤–"
            
            with st.chat_message(role, avatar=avatar):
                if role == "user":
                    st.caption(f"{msg_persona}")
                
                # Sanitize content
                import re
                display_content = re.sub(r'<suggestions>.*?</suggestions>', '', msg["content"], flags=re.DOTALL).strip()
                st.markdown(display_content)

                # Capture suggestions (only from model)
                if role == "model":
                    match = re.search(r'<suggestions>(.*?)</suggestions>', msg["content"], flags=re.DOTALL)
                    if match:
                        try:
                            st.session_state._temp_suggestions = json.loads(match.group(1))
                        except:
                            pass
                            pass
    # Reset temp suggestions
    if "_temp_suggestions" in st.session_state:
        del st.session_state["_temp_suggestions"]

    # --- Auto-Scroll Logic ---
    # Check if a new message has been added
    current_len = len(st.session_state.ai_interviewer.history)
    last_len = st.session_state.get("last_history_len", 0)

    if current_len > last_len:
        # Inject JavaScript to scroll to the top of the last message
        js = """
        <script>
            var elements = window.parent.document.querySelectorAll('.stChatMessage');
            if (elements.length > 0) {
                var last = elements[elements.length - 1];
                last.scrollIntoView({behavior: "smooth", block: "start"});
            }
        </script>
        """
        components.html(js, height=0)
        st.session_state["last_history_len"] = current_len
    
    # Ensure baseline is set if it's the first run or reset
    if "last_history_len" not in st.session_state:
        st.session_state["last_history_len"] = current_len

    history = st.session_state.ai_interviewer.history
    loaded_count = st.session_state.get("loaded_msg_count", 0)

    # 1. Past History (Collapsible)
    if loaded_count > 0:
        with st.expander("ğŸ•’ éå»ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º (Loaded History)", expanded=False):
            for i in range(loaded_count):
                if i < len(history):
                     render_message(history[i], persona)

    # 2. New Session History
    for i in range(loaded_count, len(history)):
        render_message(history[i], persona)

    # --- Rendering Contextual Support (Hints & Examples) ---
    # Retrieve suggestions from LAST message if it was from model
    last_msg = history[-1] if history else None
    current_suggestions = {}
    
    if last_msg and last_msg["role"] == "model":
        import re
        match = re.search(r'<suggestions>(.*?)</suggestions>', last_msg["content"], flags=re.DOTALL)
        if match:
            try:
                current_suggestions = json.loads(match.group(1))
            except:
                pass

    suggested_prompt = None

    if current_suggestions:
        hints = current_suggestions.get("hints")
        example = current_suggestions.get("example")
        
        if hints or example:
            with st.container(border=True): # Distinct box for AI assistance
                st.caption("ğŸ’¡ AIã‹ã‚‰ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹")
                if hints:
                    st.info(f"**ãƒ’ãƒ³ãƒˆ**: {hints}")
                if example:
                    st.success(f"**å›ç­”ä¾‹**: {example}")
                    # Improvement: Button to use the example as answer
                    # Use stable key based on content hash AND history length to ensure uniqueness per turn
                    import hashlib
                    # Include length of history to differentiate "Yes" at turn 1 vs "Yes" at turn 5
                    unique_str = f"{example}_{len(st.session_state.ai_interviewer.history)}"
                    stable_key = hashlib.md5(unique_str.encode()).hexdigest()
                    if st.button("ğŸ“‹ å›ç­”ä¾‹ã®é€šã‚Šå›ç­”ã™ã‚‹", key=f"use_example_{stable_key}"):
                        suggested_prompt = example

    # --- Next Action Suggestions (Quick Replies) ---
    st.caption("ğŸ‘‡ ã‚¯ã‚¤ãƒƒã‚¯è¿”ä¿¡ (ã‚¯ãƒªãƒƒã‚¯ã§é€ä¿¡)")
    
    # Prioritize dynamic options
    options = current_suggestions.get("options", [])
    
    # Fallback if no dynamic options
    if not options:
        fallback_map = {
            "çµŒå–¶è€…": ["äº‹æ¥­ã®å¼·ã¿ã«ã¤ã„ã¦", "è‡ªç„¶ç½å®³ã¸ã®æ‡¸å¿µ", "é‡è¦ãªè¨­å‚™ãƒ»è³‡ç”£"],
            "å¾“æ¥­å“¡": ["ç·Šæ€¥æ™‚ã®é€£çµ¡ä½“åˆ¶", "é¿é›£çµŒè·¯ã®ç¢ºèª", "é¡§å®¢å¯¾å¿œãƒãƒ‹ãƒ¥ã‚¢ãƒ«"],
            "å•†å·¥ä¼šè·å“¡": ["ãƒã‚¶ãƒ¼ãƒ‰ãƒãƒƒãƒ—ç¢ºèª", "æå®³ä¿é™ºã®åŠ å…¥çŠ¶æ³", "åœ°åŸŸé˜²ç½è¨ˆç”»ã¨ã®é€£æº"]
        }
        options = fallback_map.get(persona, [])

    # Render Options
    if options:
        cols = st.columns(min(len(options), 4))
        for i, opt in enumerate(options[:4]):
            # Use stable key based on option content and index to prevent state loss
            if cols[i].button(opt, use_container_width=True, key=f"quick_reply_{i}_{opt}"):
                suggested_prompt = opt
                          # Since it's nested Pydantic, this is non-trivial without a proper recursive merge.
                          # Plan B: Just store it in "latest_extracted" and let the Dashboard "Analyze" button handle the full merge?
                          # No, user wants immediate effect.
                          
                          # Validating directly
                          # Note: logic here is risky without deep matching.
                          # For this iteration, let's just toast that we 'Understood' and rely on the AI's short-term memory (Context)
                          # because sending it to the chat history (which we do below) is the primary way the Chat AI knows about it.
                          # The "Structuring" happens when we hit "Analyze" or when we export.
                          # BUT, the user said "Gemini 3.0... text to it... then Gemini 2.5".
                          # The `extract_structured_data` USES a separate model (implied).
                          # AND we inject the result back into history?
                          pass
                          
                  except Exception as e:
                      print(f"Extraction failed: {e}")
                      status.update(label="âš ï¸ Extraction skipped", state="error")
        
        # Determine who responds: Model or just UI update (Wait, logic flow check)
        # The structure here is: if we have a prompt (user input or suggestion), we send it.
        
        with st.chat_message("model", avatar="ğŸ¤–"):
            with st.spinner("AI is thinking..."):
                response = st.session_state.ai_interviewer.send_message(
                    final_prompt, 
                    persona=persona,
                    user_data=user_data
                )
                st.markdown(response)
                
                # --- Auto-Save Hook ---
                perform_auto_save()
                
                # Feedback Toast
                st.toast("ğŸ“ ä¼šè©±ãƒ­ã‚°ã‚’æ›´æ–°ã—ã¾ã—ãŸ (Log Saved)", icon="âœ…")
                time.sleep(1) # Wait for toast to be seen briefly
                st.rerun()
elif mode == "Dashboard Mode (Progress)":
    # Navigation Header for Dashboard
    col_dash_head1, col_dash_head2 = st.columns([3, 1])
    with col_dash_head1:
        st.title("ğŸ“Š Progress Dashboard")
    with col_dash_head2:
        # 3-Way Back Navigation
        st.button("â¬…ï¸ çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", on_click=change_mode, args=("Chat Mode (Interview)", "çµŒå–¶è€…"), use_container_width=True)
        st.button("â¬…ï¸ å¾“æ¥­å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", on_click=change_mode, args=("Chat Mode (Interview)", "å¾“æ¥­å“¡"), use_container_width=True)
        st.button("â¬…ï¸ å•†å·¥ä¼šè·å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", on_click=change_mode, args=("Chat Mode (Interview)", "å•†å·¥ä¼šè·å“¡"), use_container_width=True)

    st.info("ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‹ã‚‰äº‹æ¥­è¨ˆç”»æ›¸ã®å®Œæˆåº¦ã‚’è‡ªå‹•åˆ¤å®šã—ã¾ã™ã€‚")
    
    from src.api.schemas import ApplicationRoot
    
    # è§£æå®Ÿè¡Œãƒœã‚¿ãƒ³
    if st.button("è§£æã™ã‚‹", type="primary", use_container_width=True):
        st.info("ğŸš€ Process Started: Checking Modules...")
        
        # ã‚¹ãƒ”ãƒŠãƒ¼ã‚’ä½¿ã‚ãšã«é€æ¬¡å®Ÿè¡Œã‚’è¡¨ç¤º
        status_placeholder = st.empty()
        
        try:
            status_placeholder.text("â³ Importing Schema...")
            from src.api.schemas import ApplicationRoot
            
            status_placeholder.text("â³ Calling Gemini API (This may take 10-20s)...")
            extracted_data = st.session_state.ai_interviewer.analyze_history()
            
            status_placeholder.text(f"âœ… API Returned. Data Type: {type(extracted_data)}")
            
            if extracted_data:
                status_placeholder.text("â³ Validating data with Pydantic...")
                try:
                    # Robust Migration for Legacy/Loose Formats
                    migrated = ApplicationRoot.migrate_legacy_data(extracted_data)
                    plan = ApplicationRoot.model_validate(migrated)
                    
                    st.session_state.current_plan = plan
                    status_placeholder.success("ğŸ‰ Analysis Complete & Plan Updated!")
                    
                    # Auto-Save after Analysis
                    perform_auto_save()
                except Exception as val_e:
                    status_placeholder.error(f"Validation Error: {val_e}")
                    st.json(extracted_data)
                    st.stop()
                
                # --- Quality Check & Logic (Pending Migration) ---
                # issues = plan.check_quality()
                # missing_fields = []
                # if issues: ...
                
                st.session_state.ai_interviewer.set_focus_fields([]) # Clear focus for now
                
                time.sleep(1)
                st.rerun()

            else:
                status_placeholder.warning("âš ï¸ No data extracted (Empty result received).")

        except Exception as e:
            status_placeholder.error(f"âŒ Critical Error: {e}")
            st.exception(e)
    
    # è§£æçµæœã®è¡¨ç¤º (Updated for ApplicationRoot key mapping)
    if "current_plan" in st.session_state:
        plan: ApplicationRoot = st.session_state.current_plan
        from src.core.completion_checker import CompletionChecker
        
        # Run Analysis
        result = CompletionChecker.analyze(plan)
        
        # --- 1. Status Banner & Header ---
        st.divider()
        st.subheader("ğŸ“Š Plan Progress Dashboard")
        
        col_m1, col_m2 = st.columns([1, 4])
        with col_m1:
            st.metric(label="èªå®šå¯èƒ½æ€§ã‚¹ã‚³ã‚¢ (Score)", value=f"{result['total_score']} / 100", help="100ç‚¹ã§é›»å­ç”³è«‹ã®èªå®šè¦ä»¶ã‚’æº€ãŸã—ã¾ã™")
            
        with col_m2:
            st.caption("èªå®šã«å‘ã‘ãŸå¿…é ˆé …ç›®ã®å…¥åŠ›çŠ¶æ³ (Mandatory Requirements)")
            st.progress(result['mandatory_progress'])
            st.caption(f"å¿…é ˆé …ç›®ã®é”æˆç‡: {int(result['mandatory_progress']*100)}% å®Œäº†")
            
        # --- 2. Actionable Alerts (Missing Mandatory) - SEVERITY-BASED ---
        if result['status'] != "success":
            with st.container(border=True): # Red/Error container simulation
                st.error("ğŸš¨ ç”³è«‹ã«å‘ã‘ã¦ã€ä»¥ä¸‹ã®å¿…é ˆé …ç›®ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                # Define a mapping for section names to Japanese
                section_map = {
                    "BasicInfo": "åŸºæœ¬æƒ…å ±",
                    "Goals": "äº‹æ¥­æ¦‚è¦ãƒ»ç›®æ¨™",
                    "ResponseProcedures": "åˆå‹•å¯¾å¿œ",
                    "Measures": "äº‹å‰å¯¾ç­–",
                    "FinancialPlan": "è³‡é‡‘è¨ˆç”»",
                    "PDCA": "æ¨é€²ä½“åˆ¶ (PDCA)"
                }
                
                # ... (Logic omitted for brevity in tool call, but context needs to match) ...
                # Group by severity for clearer display
                critical_items = [m for m in result['missing_mandatory'] if m.get('severity') == 'critical']
                warning_items = [m for m in result['missing_mandatory'] if m.get('severity') == 'warning']
                
                if critical_items:
                    st.markdown("### ğŸ”´ **Critical (æœªå…¥åŠ›)**")
                    for item in critical_items:
                        sec_label = section_map.get(item['section'], item['section'])
                        st.error(f"**{sec_label}**: {item['msg']}", icon="ğŸ”´")
                
                if warning_items:
                    st.markdown("### ğŸŸ¡ **Warning (å…¥åŠ›ä¸è¶³)**")
                    for item in warning_items:
                        sec_label = section_map.get(item['section'], item['section'])
                        st.warning(f"**{sec_label}**: {item['msg']}", icon="ğŸŸ¡")
                
                with st.columns(2)[0]:
                    if st.button("ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ã‚¢ãƒ¼ã«ä¸è¶³é …ç›®ã‚’èã„ã¦ã‚‚ã‚‰ã†", type="primary", key="btn_ask_missing"):
                        missing_msgs = [m['msg'] for m in result['missing_mandatory']]
                        st.session_state.ai_interviewer.set_focus_fields(missing_msgs)
                        st.session_state.app_nav_selection = st.session_state.get("last_chat_nav", "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼")
                        # Set flag to auto-start conversation on redirect
                        st.session_state.auto_trigger_message = "ç¾åœ¨ã€ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ç¢ºèªã—ãŸä¸è¶³é …ç›®ï¼ˆfocus_fieldsï¼‰ã«ã¤ã„ã¦ã€å…·ä½“çš„ãªè³ªå•ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«é¸æŠè‚¢ã‚’æç¤ºã—ã€å›ç­”ã—ã‚„ã™ãã—ã¦ãã ã•ã„ã€‚"
                        st.session_state.auto_trigger_persona = st.session_state.get("last_chat_nav", "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼").replace("ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", "") # Rough parse
                        st.rerun()

        elif result['recommended_progress'] < 1.0:
            st.success("âœ… ç”³è«‹è¦ä»¶ã¯ã‚¯ãƒªã‚¢ã—ã¦ã„ã¾ã™ï¼ (ã•ã‚‰ã«è¨ˆç”»ã‚’å¼·åŒ–ã—ã¾ã—ã‚‡ã†)")
            with st.expander("ğŸ’¡ ã•ã‚‰ãªã‚‹å“è³ªå‘ä¸Šã®ãƒ’ãƒ³ãƒˆ (Recommended Actions)", expanded=True):
                for sug in result['suggestions']:
                    st.info(f"Suggestion: {sug}")

        else:
             st.balloons()
             st.success("ğŸ† Perfect! è¨ˆç”»ã¯å®Œç’§ã§ã™ã€‚ç”³è«‹ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚")
        
        st.divider()
        col_exp1, col_exp2 = st.columns([3, 1])
        with col_exp2:
            # Excel Export
            if st.button("ğŸ“„ ä¸‹æ›¸ãã‚·ãƒ¼ãƒˆå‡ºåŠ› (Excel)", key="btn_export_draft", use_container_width=True):
                try:
                    from src.core.draft_exporter import DraftExporter
                    excel_data = DraftExporter.export_to_excel(plan, result)
                    st.download_button(
                        label="â¬‡ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹",
                        data=excel_data,
                        file_name=f"jigyokei_draft_{plan.basic_info.corporate_name or 'plan'}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        key="btn_download_excel_real"
                    )
                    st.success("Excelç”Ÿæˆå®Œäº†ï¼ä¸Šã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
                except ImportError as ie:
                     st.error(f"ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä¸è¶³: {ie} (pip install openpyxl ãŒå¿…è¦ã§ã™)")
                except Exception as e:
                    st.error(f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

            # JSON Export (For Commerce Society / Backup)
            st.divider()
            json_str = plan.model_dump_json(indent=2)
            st.download_button(
                label="ğŸ’¾ è¨ˆç”»ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ (JSON)",
                data=json_str,
                file_name=f"jigyokei_data_{plan.basic_info.corporate_name or 'plan'}.json",
                mime="application/json",
                help="å•†å·¥ä¼šé€£æºç”¨ã€ã¾ãŸã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚",
                use_container_width=True
            )

        # --- 3. Section Breakdown (Application Form Style: 6 Tabs) ---
        st.divider()
        
        # Dynamic Tab Labels (6-tab structure matching electronic application)
        tabs_labels = {
            "BasicInfo": "1ï¸âƒ£ åŸºæœ¬æƒ…å ±",
            "Goals": "2ï¸âƒ£ äº‹æ¥­æ¦‚è¦ãƒ»ç›®æ¨™",
            "Disaster": "3ï¸âƒ£ ç½å®³æƒ³å®š",
            "Response": "4ï¸âƒ£ åˆå‹•å¯¾å¿œ",
            "Measures": "5ï¸âƒ£ äº‹å‰å¯¾ç­–",
            "Finance": "6ï¸âƒ£ è³‡é‡‘ãƒ»æ¨é€²ä½“åˆ¶"
        }
        
        # Check missing items to add warning icons
        missing_sections = [m['section'] for m in result['missing_mandatory']]
        
        if "BasicInfo" in missing_sections: tabs_labels["BasicInfo"] += " âš ï¸"
        if "Goals" in missing_sections: tabs_labels["Goals"] += " âš ï¸"
        if "Goals" in missing_sections: tabs_labels["Disaster"] += " âš ï¸"  # Disaster is part of Goals
        if "ResponseProcedures" in missing_sections: tabs_labels["Response"] += " âš ï¸"
        if "Measures" in missing_sections: tabs_labels["Measures"] += " âš ï¸"
        if "FinancialPlan" in missing_sections or "PDCA" in missing_sections: tabs_labels["Finance"] += " âš ï¸"

        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
            tabs_labels["BasicInfo"], 
            tabs_labels["Goals"], 
            tabs_labels["Disaster"],
            tabs_labels["Response"],
            tabs_labels["Measures"],
            tabs_labels["Finance"]
        ])
        
        # TAB 1: Basic Info
        with tab1:
            st.caption("ğŸ“‹ æ§˜å¼ç¬¬1 åŸºæœ¬æƒ…å ±")
            if plan.basic_info:
                bi = plan.basic_info
                full_address = f"{bi.address_pref or ''}{bi.address_city or ''}{bi.address_street or ''}{bi.address_building or ''}"
                
                display_data = {
                    "ä¼šç¤¾å": bi.corporate_name,
                    "ä»£è¡¨è€…": f"{bi.representative_title or ''} {bi.representative_name or ''}".strip(),
                    "è³‡æœ¬é‡‘": f"{bi.capital:,}å††" if bi.capital else "-",
                    "å¾“æ¥­å“¡æ•°": f"{bi.employees}å" if bi.employees else "-",
                    "éƒµä¾¿ç•ªå·": bi.address_zip,
                    "ä½æ‰€": full_address,
                    "æ¥­ç¨®": f"{bi.industry_major or ''} / {bi.industry_middle or ''}".strip(" /"),
                    "æ³•äººç•ªå·": bi.corporate_number or "-"
                }
                st.table([{"é …ç›®": k, "å†…å®¹": v} for k, v in display_data.items() if v and v != "-"])
            else:
                with st.container(border=True):
                    st.warning("âš ï¸ åŸºæœ¬æƒ…å ±ãŒæœªå…¥åŠ›ã§ã™ã€‚")
        
        # TAB 2: Overview & Goals
        with tab2:
            st.caption("ğŸ“‹ æ§˜å¼ç¬¬2 äº‹æ¥­æ´»å‹•ã®æ¦‚è¦ãƒ»å–çµ„ç›®çš„")
            
            with st.container(border=True):
                st.subheader("äº‹æ¥­æ´»å‹•ã®æ¦‚è¦")
                if plan.goals.business_overview:
                    st.info(plan.goals.business_overview)
                else:
                    st.error("ğŸš¨ äº‹æ¥­æ´»å‹•ã®æ¦‚è¦ãŒæœªå…¥åŠ›ã§ã™ã€‚")
                    st.caption("è‡ªç¤¾ã®äº‹æ¥­å†…å®¹ã€ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³ä¸Šã®å½¹å‰²ã€åœ°åŸŸçµŒæ¸ˆã¸ã®è²¢çŒ®ã‚’å…·ä½“çš„ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚")
            
            with st.container(border=True):
                st.subheader("å–çµ„ç›®çš„")
                if plan.goals.business_purpose:
                    st.info(plan.goals.business_purpose)
                else:
                    st.warning("âš ï¸ å–çµ„ç›®çš„ãŒæœªå…¥åŠ›ã§ã™ã€‚")
        
        # TAB 3: Disaster Scenario
        with tab3:
            st.caption("ğŸ“‹ æ§˜å¼ç¬¬3 äº‹æ¥­æ´»å‹•ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹è‡ªç„¶ç½å®³ç­‰ã®æƒ³å®š")
            
            with st.container(border=True):
                st.subheader("æƒ³å®šã™ã‚‹è‡ªç„¶ç½å®³ç­‰")
                if plan.goals.disaster_scenario.disaster_assumption:
                    st.info(plan.goals.disaster_scenario.disaster_assumption)
                else:
                    st.error("ğŸš¨ ç½å®³æƒ³å®šãŒæœªå…¥åŠ›ã§ã™ã€‚")
                    st.caption("ãƒã‚¶ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã‚’å‚ç…§ã—ã€ã€Œéœ‡åº¦â—‹â—‹ã€ã€Œæµ¸æ°´æ·±â—‹â—‹mã€ãªã©å…·ä½“çš„ãªæ•°å€¤ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚")
            
            # New Impact Structure Display
            st.subheader("è‡ªç„¶ç½å®³ç­‰ã®ç™ºç”ŸãŒäº‹æ¥­æ´»å‹•ã«ä¸ãˆã‚‹å½±éŸ¿")
            imp = plan.goals.disaster_scenario.impacts
            impact_data = {
                "äººå“¡": imp.impact_personnel,
                "å»ºç‰©ãƒ»è¨­å‚™": imp.impact_building,
                "è³‡é‡‘ç¹°ã‚Š": imp.impact_funds,
                "æƒ…å ±": imp.impact_info
            }
            # Filter non-empty
            impact_rows = [{"é …ç›®": k, "å†…å®¹": v} for k, v in impact_data.items() if v]
            if impact_rows:
                st.table(impact_rows)
            else:
                st.warning("âš ï¸ å½±éŸ¿è©³ç´°ãŒæœªå…¥åŠ›ã§ã™ã€‚")
        
        # TAB 4: First Response
        with tab4:
            st.caption(f"ğŸ“‹ æ§˜å¼ç¬¬4 åˆå‹•å¯¾å¿œæ‰‹é †ç­‰: {len(plan.response_procedures)}ä»¶ç™»éŒ²æ¸ˆ")
            if plan.response_procedures:
                st.table([m.model_dump() for m in plan.response_procedures])
            else:
                with st.container(border=True):
                    st.error("ğŸš¨ åˆå‹•å¯¾å¿œãŒæœªç™»éŒ²ã§ã™ã€‚")
                    st.caption("ç½å®³ç™ºç”Ÿç›´å¾Œã«èª°ãŒä½•ã‚’ã™ã‚‹ã‹ï¼ˆä¾‹ï¼šå®‰å¦ç¢ºèªã€é¿é›£èª˜å°ï¼‰ã‚’æ±ºã‚ã¦ãã ã•ã„ã€‚")
        
        # TAB 5: Measures (A/B/C/D)
        with tab5:
            st.caption(f"ğŸ“‹ æ§˜å¼ç¬¬5 å¹³æ™‚ã®æ¨é€²ä½“åˆ¶ (4ã‚«ãƒ†ã‚´ãƒª)")
            
            measures = plan.measures
            
            # Helper to display MeasureDetail
            def show_measure(label, item):
                with st.expander(label, expanded=True):
                    c1, c2 = st.columns(2)
                    c1.markdown("**ç¾åœ¨ã®å–çµ„**")
                    if item.current_measure: c1.info(item.current_measure)
                    else: c1.warning("æœªå…¥åŠ›")
                    
                    c2.markdown("**ä»Šå¾Œã®è¨ˆç”»**")
                    if item.future_plan: c2.success(item.future_plan)
                    else: c2.caption("ãªã—")

            show_measure("A: äººå“¡ä½“åˆ¶ã®æ•´å‚™ (ãƒ’ãƒˆ)", measures.personnel)
            show_measure("B: å»ºç‰©ãƒ»è¨­å‚™ã®ä¿å…¨ (ãƒ¢ãƒ)", measures.building)
            show_measure("C: è³‡é‡‘èª¿é”æ‰‹æ®µã®ç¢ºä¿ (ã‚«ãƒ)", measures.money)
            show_measure("D: æƒ…å ±ã®ä¿è­· (æƒ…å ±)", measures.data)

        
        # TAB 6: Finance & PDCA
        with tab6:
            st.caption("ğŸ“‹ æ§˜å¼ç¬¬6 è³‡é‡‘è¨ˆç”»ãƒ»æ¨é€²ä½“åˆ¶")
            
            with st.container(border=True):
                st.subheader("ğŸ’° è³‡é‡‘è¨ˆç”»")
                if plan.financial_plan.items:
                    st.table([i.model_dump() for i in plan.financial_plan.items])
                else:
                    st.warning("âš ï¸ è³‡é‡‘è¨ˆç”»ãŒæœªå…¥åŠ›ã§ã™ã€‚")
                    st.caption("å¾©æ—§ã«ã‹ã‹ã‚‹è²»ç”¨ã®ç›®å®‰ã¨ã€ãã®èª¿é”æ–¹æ³•ï¼ˆä¿é™ºã€è‡ªå·±è³‡é‡‘ã€å€Ÿå…¥ãªã©ï¼‰ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
            
            with st.container(border=True):
                st.subheader("ğŸ› ï¸ è¨­å‚™ãƒªã‚¹ãƒˆ (ç¨åˆ¶å„ªé‡) (ä»»æ„)")
                if plan.equipment.items:
                    st.table([i.model_dump() for i in plan.equipment.items])
                else:
                    st.info("è¨­å‚™ãƒªã‚¹ãƒˆãªã— (ä»»æ„)")
            
            with st.container(border=True):
                st.subheader("ğŸ”„ æ¨é€²ä½“åˆ¶ãƒ»è¨“ç·´")
                pdca_data = {
                    "ç®¡ç†ä½“åˆ¶": plan.pdca.management_system or "-",
                    "è¨“ç·´ãƒ»æ•™è‚²": plan.pdca.training_education or "-"
                }
                st.table([{"é …ç›®": k, "å†…å®¹": v} for k, v in pdca_data.items()])

        # --- 4. Sidebar Tools (Injected here dynamically or rely on static layout) ---
        # Note: Sidebar is already rendered at top of script. We can add to it here or just leave as is.
        # Adding a dedicated "Tools" expander in main area for visibility
        with st.expander("ğŸ› ï¸ ãŠå½¹ç«‹ã¡ãƒ„ãƒ¼ãƒ« (External Tools)"):
            c1, c2, c3 = st.columns(3)
            c1.link_button("ğŸŒ ãƒã‚¶ãƒ¼ãƒ‰ãƒãƒƒãƒ—ãƒãƒ¼ã‚¿ãƒ«", "https://disaportal.gsi.go.jp/")
            c2.link_button("ğŸ“‰ J-SHIS åœ°éœ‡äºˆæ¸¬", "https://www.j-shis.bosai.go.jp/")
            c3.link_button("ğŸ’´ BCPãƒãƒ¼ã‚¿ãƒ« (ãƒªã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒŠãƒ³ã‚¹ç­‰)", "https://kyoujinnka.smrj.go.jp/")

    else:
        st.info("â˜ï¸ Click the button to analyze current chat history.")

    st.divider()
    with st.expander("Show Raw Chat History"):
        st.json(st.session_state.ai_interviewer.history)

elif mode == "Main Consensus Room (Resolution)":
    st.title("âš–ï¸ Consensus Room (å…¨ä½“åˆæ„)")
    st.caption("å„ãƒšãƒ«ã‚½ãƒŠã®æ„è¦‹ã‚’èª¿æ•´ã—ã€æœ€çµ‚çš„ãªæ–¹é‡ã‚’æ±ºå®šã—ã¾ã™ã€‚")

    # --- File Upload for Consensus (New) ---
    with st.expander("ğŸ“‚ è³‡æ–™ã®è¿½åŠ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (Upload Documents)", expanded=False):
        uploaded_refs_consensus = st.file_uploader(
            "å…¨ä½“åˆæ„ç”¨è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF/ç”»åƒ)", 
            type=["pdf", "png", "jpg", "jpeg"], 
            accept_multiple_files=True,
            key="uploader_consensus"
        )
        
        # --- Auto-Process Logic (Consensus) ---
        if "processed_file_ids" not in st.session_state:
            st.session_state.processed_file_ids = set()

        if uploaded_refs_consensus:
            new_files_to_process = []
            for file in uploaded_refs_consensus:
                file_id = f"{file.name}_{file.size}"
                if file_id not in st.session_state.processed_file_ids:
                    new_files_to_process.append(file)
                    st.session_state.processed_file_ids.add(file_id)
            
            if new_files_to_process:
                 with st.spinner("è³‡æ–™ã‚’è§£æä¸­... (Processing for Consensus)"):
                    try:
                        # Process files as "ç·åˆèª¿æ•´å½¹" (Coordinator)
                        count = st.session_state.ai_interviewer.process_files(new_files_to_process, target_persona="ç·åˆèª¿æ•´å½¹")
                        st.success(f"{count}ä»¶ã®è³‡æ–™ã‚’å…¨ä½“åˆæ„ç”¨ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼")
                        
                        # Agentic Extraction Trigger (Optional but good for consistency)
                        if count > 0:
                             with st.status("ğŸ¤– AI Agent Working: è³‡æ–™ã‚’è©³ç´°åˆ†æä¸­...", expanded=True) as status:
                                 status.write("ğŸ“ Gemini 1.5 Pro (High Reasoning) ã§è³‡æ–™ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
                                 try:
                                     all_files = st.session_state.ai_interviewer.uploaded_file_refs
                                     extracted_data = st.session_state.ai_interviewer.extract_structured_data(text="", file_refs=all_files)
                                     if extracted_data:
                                         status.write("âœ… æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")
                                     else:
                                         status.write("â„¹ï¸ æ–°è¦ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                                 except Exception as ex_e:
                                     status.error(f"Extraction Error: {ex_e}")

                        time.sleep(1)
                        perform_auto_save()
                        st.rerun()
                    except Exception as e:
                        st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    
    # Conflict Detection
    with st.expander("ğŸ§ çŸ›ç›¾ãƒ»æœªåˆæ„äº‹é …ã®æ¤œçŸ¥ (Conflict Detection)", expanded=True):
        if st.button("çŸ›ç›¾ã‚’å†ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹", type="primary"):
            with st.spinner("Analyzing conflicts..."):
                conflicts = st.session_state.ai_interviewer.detect_conflicts()
                st.session_state._conflicts_cache = conflicts
        
        # Retrieve cache
        current_conflicts_data = st.session_state.get("_conflicts_cache", {})
        current_conflicts = current_conflicts_data.get("conflicts", [])
        
        if current_conflicts:
            st.warning(f"{len(current_conflicts)}ä»¶ã®çŸ›ç›¾ã¾ãŸã¯æœªåˆæ„äº‹é …ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
            for i, c in enumerate(current_conflicts):
                st.markdown(f"#### {i+1}. {c.get('topic', 'Topic')}")
                col1, col2 = st.columns(2)
                with col1:
                    st.info(f"**A: {c.get('persona_A')}**\n\n{c.get('statement_A')}")
                with col2:
                    st.info(f"**B: {c.get('persona_B')}**\n\n{c.get('statement_B')}")
                st.success(f"ğŸ’¡ **AI Suggestion**: {c.get('suggestion')}")
                st.divider()
        else:
             st.info("çŸ›ç›¾ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ (æœªã‚¹ã‚­ãƒ£ãƒ³ã¾ãŸã¯è§£æ¶ˆæ¸ˆã¿)")

    st.divider()
    st.subheader("ğŸ’¬ å…¨ä½“æ–¹é‡ã®æ±ºå®š")
    
    # Chat History
    history = st.session_state.ai_interviewer.history
    
    # Helper for rendering messages in Consensus Mode (Duplicate of Chat Mode helper to avoid scope issues)
    def render_message_consensus(msg, current_persona):
        if not isinstance(msg, dict): return
        role = msg["role"]
        msg_persona = msg.get("persona", "Unknown")
        target_persona = msg.get("target_persona")
        
        # In Consensus, we generally want to see everything, OR filter by "General" context.
        # However, to be safe and match user expectation: show messages relevant to 'ç·åˆèª¿æ•´å½¹' or public.
        # Let's show everything for now as it is a "Consensus" room.
        # But if we want to be strict: 
        visible = True # Default to visible in Consensus
        # Apply filter if needed:
        # if role == "model" and target_persona and target_persona != "ç·åˆèª¿æ•´å½¹": visible = False
        
        if visible:
            avatar = "ğŸ¤–" if role == "model" else "ğŸ‘¤"
            if msg_persona == "çµŒå–¶è€…": avatar = "ğŸ‘¨â€ğŸ’¼"
            elif msg_persona == "å¾“æ¥­å“¡": avatar = "ğŸ‘·"
            elif msg_persona == "å•†å·¥ä¼šè·å“¡": avatar = "ğŸ§‘â€ğŸ«"
            elif msg_persona == "AI Concierge": avatar = "ğŸ¤–"
            elif msg_persona == "ç·åˆèª¿æ•´å½¹": avatar = "âš–ï¸"
            
            with st.chat_message(role, avatar=avatar):
                st.caption(f"{msg_persona}")
                st.markdown(msg["content"])

    # Show history using rendered helper
    for i in range(len(history)):
         render_message_consensus(history[i], "ç·åˆèª¿æ•´å½¹") 
    
    # Input
    if prompt := st.chat_input("å…¨ä½“æ–¹é‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (ä¾‹: é¿é›£å ´æ‰€ã¯é«˜å°ã®å…¬åœ’ã¨ã—ã¾ã™)"):
         with st.chat_message("user", avatar="ğŸ‘¤"):
            st.markdown(prompt)
         
         # Metadata
         user_name = st.session_state.get("user_name_input", "")
         user_position = st.session_state.get("user_position_input", "")
         user_data = {"name": user_name, "position": user_position}
         
         with st.chat_message("model", avatar="ğŸ¤–"):
            with st.spinner("AI Facilitator is recording..."):
                response = st.session_state.ai_interviewer.send_message(
                    prompt, 
                    persona="ç·åˆèª¿æ•´å½¹",
                    user_data=user_data
                )
                st.markdown(response)
                perform_auto_save()
                st.rerun()
