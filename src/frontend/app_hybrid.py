import sys
import os
import streamlit as st
import json
import time
import importlib
import streamlit.components.v1 as components

# --- Page Config (Must be the first Streamlit command) ---
st.set_page_config(
    page_title="Jigyokei Hybrid System",
    page_icon="ğŸ‘‘",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- Check Reset Status ---
if "action" in st.query_params and st.query_params["action"] == "reset":
    st.toast("ğŸ—‘ï¸ ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ (Reset Complete)", icon="âœ…")
    st.query_params.clear()

# --- Custom CSS for Mobile UI ---
st.markdown("""
<style>
    /* Customize Sidebar Toggle (Expanded/Collapsed Control) */
    /* Target Desktop Collapsed Control */
    [data-testid="stSidebarCollapsedControl"] {
        background-color: #ffeaea !important; 
        border: 2px solid #ff4b4b !important;
        border-radius: 8px !important;
        padding: 2px !important;
        width: 44px !important;
        height: auto !important;
        min_height: 80px !important; /* Make it tall and noticeable */
        display: flex !important;
        align-items: center !important;
        justify-content: center !important;
        z-index: 1000002 !important; /* Higher than Streamlit header overlay */
        opacity: 1 !important;
        visibility: visible !important;
    }

    /* Target Mobile Header Button (Often behaves differently) */
    @media (max-width: 768px) {
        /* On mobile, the toggle might be in the header. 
           We target the first button in the header if specific ID fails, 
           NOTE: Streamlit mobile often uses stSidebarCollapsedControl even in header, 
           but sometimes it is just a button in stHeader. */
        
        [data-testid="stHeader"] button[data-testid="stSidebarCollapsedControl"] {
             background-color: #ffeaea !important;
             border: 2px solid #ff4b4b !important;
             width: auto !important; /* Allow width to expand for text */
             height: auto !important;
             min-height: 44px !important;
             aspect-ratio: auto !important;
             border-radius: 8px !important;
        }
        
        /* Adjust text for mobile (Horizontal 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼') */
        [data-testid="stHeader"] button[data-testid="stSidebarCollapsedControl"]::after {
            content: "ãƒ¡ãƒ‹ãƒ¥ãƒ¼" !important;
            writing-mode: horizontal-tb !important;
            font-size: 16px !important;
            padding: 0 8px !important;
            letter-spacing: 1px !important;
        }
    }

    /* Hide the default '>>' or 'hamburger' icon */
    [data-testid="stSidebarCollapsedControl"] svg, 
    [data-testid="stSidebarCollapsedControl"] img {
        display: none !important;
    }
    
    /* Add 'ãƒ¡ãƒ‹ãƒ¥ãƒ¼' label (Default Vertical for Desktop Sidebar) */
    [data-testid="stSidebarCollapsedControl"]::after {
        content: "ãƒ¡ãƒ‹ãƒ¥ãƒ¼";
        font-family: "Hiragino Sans", "Meiryo", sans-serif;
        font-size: 14px !important;
        font-weight: 900 !important;
        color: #ff4b4b !important;
        writing-mode: vertical-rl;
        text-orientation: upright;
        letter-spacing: 2px;
        white-space: nowrap;
        display: block !important;
    }

    /* --- Sidebar Close Button Improvement --- */
    /* Target the close button inside the sidebar (header button) */
    section[data-testid="stSidebar"] button[kind="header"],
    section[data-testid="stSidebar"] [data-testid="stBaseButton-header"] {
        visibility: visible !important;
        opacity: 1 !important;
        background-color: #ffeaea !important;
        border: 2px solid #ff4b4b !important;
        border-radius: 8px !important;
        color: #ff4b4b !important; /* Icon color */
        min-width: 44px !important;
        min-height: 44px !important;
        display: flex !important;
        align-items: center !important;
        justify-content: center !important;
        margin-right: 10px !important; /* Spacing from edge */
        transition: none !important; /* Remove fade effect */
    }

    /* Add "é–‰ã˜ã‚‹" Label */
    section[data-testid="stSidebar"] button[kind="header"]::before,
    section[data-testid="stSidebar"] [data-testid="stBaseButton-header"]::before {
        content: "é–‰ã˜ã‚‹" !important;
        font-size: 12px !important;
        font-weight: bold !important;
        color: #ff4b4b !important;
        margin-right: 4px !important;
        display: inline-block !important;
    }
    
    /* Ensure icon is visible */
    section[data-testid="stSidebar"] button[kind="header"] svg,
    section[data-testid="stSidebar"] [data-testid="stBaseButton-header"] svg {
        display: block !important;
        font-weight: bold !important;
    }
""", unsafe_allow_html=True)

# --- Path Setup ---
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

# --- Module Reloading for Streamlit Cloud ---
import src.core.jigyokei_core
import src.api.schemas
import src.core.completion_checker
import src.core.draft_exporter
importlib.reload(src.core.jigyokei_core)
importlib.reload(src.api.schemas)
importlib.reload(src.core.completion_checker)
importlib.reload(src.core.draft_exporter)

from src.core.jigyokei_core import AIInterviewer
from src.data.context_loader import ContextLoader
from src.core.completion_checker import CompletionChecker
from src.core.session_manager import SessionManager

# --- Version Control ---
APP_VERSION = "3.4.2-mobile-ui-polish"

# Initialize Session Manager
if "session_manager" not in st.session_state:
    st.session_state.session_manager = SessionManager()

# --- Auto Resume Logic ---
# [DISABLED] Automatic loading of shared session file causes data leak between users in Cloud environment.
# Note: Mobile persistence is handled via scoped 'mobile_autosave' logic below.


if "app_version" not in st.session_state or st.session_state.app_version != APP_VERSION:
    st.session_state.clear()
    st.session_state.app_version = APP_VERSION
    st.rerun()

# --- Debug / Reset Controls ---
with st.sidebar:
    with st.expander("ğŸ”§ System Menu", expanded=False):
        if st.button("ğŸ—‘ï¸ Reset All Data", key="btn_hard_reset", type="primary", help="è­¦å‘Š: ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦åˆæœŸåŒ–ã—ã¾ã™"):
            # 1. Clear persistence (Disk)
            if "session_manager" in st.session_state:
                st.session_state.session_manager.clear_session()
            
            # 2. Clear Session State (Memory)
            st.session_state.clear()
            
            # 3. Notification & Rerun
            # We set a query param to show the toast after reload
            st.query_params["action"] = "reset"
            st.rerun()

# --- Initialize Managers (Standard) ---
if "ai_interviewer" not in st.session_state:
    st.session_state.ai_interviewer = AIInterviewer()
else:
    # Check for outdated instance (missing 'analyze_history')
    if not hasattr(st.session_state.ai_interviewer, "analyze_history"):
        st.warning("ğŸ”„ Upgrading AI Brain to latest version...")
        
        # Preserve old history
        old_history = getattr(st.session_state.ai_interviewer, "history", [])
        
        # Re-initialize with new class definition
        st.session_state.ai_interviewer = AIInterviewer()
        
        # Restore history logic... (simplified for this block, as load_history handles it)
        st.session_state.ai_interviewer.load_history(old_history)
        if hasattr(st.session_state.ai_interviewer, "load_history"):
             st.session_state.ai_interviewer.load_history(old_history)
        else:
             st.session_state.ai_interviewer.history = old_history
             
        st.success("âœ… AI Brain Upgraded! Please reload one last time.")
        time.sleep(5)
        st.rerun()

if "context_loader" not in st.session_state:
    root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
    context_dir = os.path.join(root_dir, "data", "context")
    st.session_state.context_loader = ContextLoader(context_dir)

# --- Helper: Auto-Save ---
def perform_auto_save():
    """
    Save the current session state (history & plan) to local storage.
    Used for mobile persistence and crash recovery.
    """
    if "session_manager" in st.session_state and "ai_interviewer" in st.session_state:
        # Prepare plan data if exists
        plan_data = None
        if "current_plan" in st.session_state and st.session_state.current_plan:
            try:
                plan_data = st.session_state.current_plan.model_dump()
            except:
                pass # Fail silently on serialization

        # Save to 'mobile_autosave' slot
        st.session_state.session_manager.save_session(
            history=st.session_state.ai_interviewer.history,
            current_plan_dict=plan_data,
            session_id="mobile_autosave"
        )

# --- Incremental Update Logic (Smart Mapper & Deep Merge) ---
class SmartUpdateMapper:
    """
    Translates simplified AI update JSON into strict Pydantic Schema structure.
    Allows AI to use simple keys like 'human_safety' instead of complex list objects.
    """
    @staticmethod
    def map_response_procedures(simple_dict):
        """Maps flat keys to ResponseProcedures list items."""
        mapped_items = []
        
        # Mapping Rules: Simple Key -> (Category, Action Content)
        # Note: We append to existing likely, or overwrite specific categories?
        # Strategy: We construct objects. Merging logic handles the rest? 
        # No, replacing list items by category is hard with deep_merge.
        # Strategy: Return a LIST of dicts that matches the structure. 
        # But deep_update on lists usually appends or overwrites index. 
        # HACK: We will load current plan, find the matching item, and update its content.
        
        # Actually, let's just return the logic for `apply_incremental_update` to handle.
        # This mapper will return a "Standardized Dict" that matches the Schema structure
        # as much as possible, or return specific instructions.
        pass

def deep_update(base_dict, update_dict):
    """Recursively update dict."""
    import collections.abc
    for k, v in update_dict.items():
        if isinstance(v, collections.abc.Mapping):
            base_dict[k] = deep_update(base_dict.get(k, {}), v)
        else:
            base_dict[k] = v
    return base_dict

def apply_incremental_update(update_json):
    """
    Apply a partial JSON update with Smart Mapping.
    """
    try:
        if "current_plan" not in st.session_state or not st.session_state.current_plan:
             from src.api.schemas import ApplicationRoot
             st.session_state.current_plan = ApplicationRoot()
        
        plan = st.session_state.current_plan
        
        # --- SMART MAPPING LOGIC (Manual Handling for Complex Lists) ---
        
        # 1. Response Procedures (List handling)
        if "response_procedures" in update_json:
            rp_data = update_json["response_procedures"]
            # Map simplified keys to specific list items
            # We assume the plan already has the 4 fixed items (initialized or empty)
            # If not, we create them? Schema defaults to empty list. 
            # Better to find by category or create.
            
            # Helper to find or create
            def update_proc(category, content):
                # Find existing
                found = False
                if not plan.response_procedures: plan.response_procedures = []
                for p in plan.response_procedures:
                    if p.category == category:
                        p.action_content = content
                        found = True
                        break
                if not found:
                    from src.api.schemas import FirstResponse
                    plan.response_procedures.append(FirstResponse(category=category, action_content=content, timing="ç™ºç½ç›´å¾Œ"))

            if isinstance(rp_data, dict):
                if "human_safety" in rp_data:
                    # Split into Evacuation and Confirmation? AI prompt said "human_safety" as one?
                    # Wait, prompt example showed output splitting? 
                    # Actually prompt example in previous turn showed: 
                    # "human_safety": "..." (Combined?)
                    # If combined, we might put same content in both or ask AI to split?
                    # Let's put in 'Evacuation' for now or split if clear.
                    txt = rp_data["human_safety"]
                    update_proc("1. äººå‘½ã®å®‰å…¨ç¢ºä¿", txt) 
                    # ideally we want specific keys. Prompt update will enforce 'evacuation' and 'safety_check' keys next.
                    
                if "evacuation" in rp_data: update_proc("1. äººå‘½ã®å®‰å…¨ç¢ºä¿", rp_data["evacuation"]) # Specific
                if "safety_check" in rp_data: update_proc("1. äººå‘½ã®å®‰å…¨ç¢ºä¿", rp_data["safety_check"]) # Specific (needs differentiating? Category name is same)
                # Actually Schema allows duplicate Category names. 
                # To distinguish: We check content or rely on order? 
                # Let's just update the *first* match for 'Evacuation' and *second* for 'Safety Check' if strictly ordered?
                # Risky. 
                # SAFER STRATEGY: Update Prompt to use EXACT keys matching Schema is best, 
                # BUT user wants "Simple".
                # Let's map "emergency_structure" -> "2. éå¸¸æ™‚ã®ç·Šæ€¥æ™‚ä½“åˆ¶ã®æ§‹ç¯‰"
                if "emergency_structure" in rp_data: update_proc("2. éå¸¸æ™‚ã®ç·Šæ€¥æ™‚ä½“åˆ¶ã®æ§‹ç¯‰", rp_data["emergency_structure"])
                if "damage_assessment" in rp_data: update_proc("3. è¢«å®³çŠ¶æ³ã®æŠŠæ¡ãƒ»è¢«å®³æƒ…å ±ã®å…±æœ‰", rp_data["damage_assessment"])
            
            # Remove from update_json so deep_update doesn't overwrite the whole list with a dict
            del update_json["response_procedures"]

        # 2. Financial Plan (List handling)
        if "finance_plan" in update_json:
            fp_data = update_json["finance_plan"] # { estimated_amount, source, details }
            # Construct a single item for now
            if not plan.financial_plan.items: plan.financial_plan.items = []
            
            # Create a "summary" item
            from src.api.schemas import FinancialPlanItem
            item_content = fp_data.get("details", "è³‡é‡‘å¯¾ç­–")
            amount = fp_data.get("estimated_amount", 0)
            method = fp_data.get("source", "")
            
            # Upsert logic: if item exists, update it, else append
            if plan.financial_plan.items:
                plan.financial_plan.items[0].item = item_content
                plan.financial_plan.items[0].amount = amount
                plan.financial_plan.items[0].method = method
            else:
                plan.financial_plan.items.append(FinancialPlanItem(item=item_content, amount=amount, method=method))
            
            del update_json["finance_plan"]
            
        # 3. PDCA (Implementation System)
        if "implementation_system" in update_json:
            pdca = update_json["implementation_system"]
            if "training_review" in pdca:
                plan.pdca.training_education = pdca["training_review"]
                plan.pdca.plan_review = pdca["training_review"] # Map to both for robust
            
            del update_json["implementation_system"]
            
        # 4. Contact Info
        if "contact_info" in update_json:
            ci = update_json["contact_info"]
            plan.applicant_info.contact_name = ci.get("name")
            plan.applicant_info.email = ci.get("email")
            plan.applicant_info.phone = ci.get("phone")
            del update_json["contact_info"]

        # --- Revert to Dict and Validate ---
        # Apply remaining simple updates (basic_info, measures etc.)
        current_dump = plan.model_dump()
        merged = deep_update(current_dump, update_json)
        
        # Save back
        from src.api.schemas import ApplicationRoot
        st.session_state.current_plan = ApplicationRoot(**merged)
        return True

    except Exception as e:
        print(f"Smart Update Failed: {e}")
        st.toast(f"âš ï¸ æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}", icon="ğŸ›") # Debug info
        return False

# --- Authentication ---
def check_password():
    if st.session_state.get("password_correct", False):
        return True

    def password_entered():
        if st.session_state["password"] == st.secrets["APP_PASSWORD"]:
            st.session_state["password_correct"] = True
            del st.session_state["password"]
        else:
            st.session_state["password_correct"] = False

    st.text_input("Password", type="password", on_change=password_entered, key="password")
    return False

if not check_password():
    st.stop()

# ==========================================
# Main App Logic
# ==========================================

# State Transition Helper
def change_mode(mode_name, persona_name=None):
    # Map legacy args to new nav selection
    target = "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼" # Default
    
    if mode_name == "Chat Mode (Interview)":
        if persona_name == "çµŒå–¶è€…": target = "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼"
        elif persona_name == "å¾“æ¥­å“¡": target = "å¾“æ¥­å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼"
        elif persona_name == "å•†å·¥ä¼šè·å“¡": target = "å•†å·¥ä¼šè·å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼"
    elif mode_name == "Main Consensus Room (Resolution)":
        target = "Main Consensus Room (å…¨ä½“åˆæ„)"
    elif mode_name == "Dashboard Mode (Progress)":
        target = "Dashboard Mode (Progress)"
         
    st.session_state.app_nav_selection = target

# --- Flash Message System ---
def set_flash_message(message, icon="INFO"):
    """Set a message to be shown after the next rerun."""
    st.session_state.flash_toast_message = message
    st.session_state.flash_toast_icon = icon

def check_flash_message():
    """Check and display pending flash messages."""
    if "flash_toast_message" in st.session_state:
        msg_str = st.session_state.flash_toast_message
        icon = st.session_state.get("flash_toast_icon", "INFO")
        
        # Support split messages for "Separate Parallel Display"
        msgs = msg_str.split("|||")
        
        for m in msgs:
            if m.strip():
                st.toast(m.strip(), icon=icon)
                time.sleep(1) # Stagger slightly
        
        # Halt execution to ensure visibility (User Requirement: 5s total)
        time.sleep(4)
        
        # Clear after showing
        del st.session_state["flash_toast_message"]
        if "flash_toast_icon" in st.session_state:
            del st.session_state["flash_toast_icon"]

def trigger_missing_items_chat():
    """Callback to trigger missing items chat flow securely before rerun."""
    # We need to access result, so we'll grab it from session state or re-calculate?
    # Actually, we can pass args to callback.
    pass

# Helper to be used in button args
def on_click_ask_missing(missing_msgs):
    st.session_state.ai_interviewer.set_focus_fields(missing_msgs)
    st.session_state.app_nav_selection = st.session_state.get("last_chat_nav", "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼")
    st.session_state.auto_trigger_message = "ä¸è¶³é …ç›®ã®å…¥åŠ›ã‚’è¡Œã„ãŸã„ã§ã™ã€‚ä½•ã‹ã‚‰å§‹ã‚ã‚Œã°ã‚ˆã„ã§ã™ã‹ï¼Ÿ"
    st.session_state.app_nav_selection = st.session_state.get("last_chat_nav", "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼")
    st.session_state.auto_trigger_message = "ä¸è¶³é …ç›®ã®å…¥åŠ›ã‚’è¡Œã„ãŸã„ã§ã™ã€‚ä½•ã‹ã‚‰å§‹ã‚ã‚Œã°ã‚ˆã„ã§ã™ã‹ï¼Ÿ"
    st.session_state.auto_trigger_persona = st.session_state.get("last_chat_nav", "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼").replace("ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", "")

def auto_complete_interview(json_str):
    """Callback to parse interview data and redirect to dashboard."""
    try:
        data_dict = json.loads(json_str)
        from src.api.schemas import ApplicationRoot
        # Migrate & Validate
        migrated = ApplicationRoot.migrate_legacy_data(data_dict)
        plan = ApplicationRoot.model_validate(migrated)
        st.session_state.current_plan = plan
        
        # Redirect
        st.session_state.app_nav_selection = "Dashboard Mode (Progress)"
        
        # Set Flash Message for next screen
        set_flash_message("âœ… è‡ªå‹•è§£æãŒå®Œäº†ã—ã¾ã—ãŸ (Auto Analysis Complete)", icon="ğŸ¤–")
        
    except Exception as e:
        st.error(f"Data Processing Error: {e}")

with st.sidebar:
    st.header("Jigyokei Hybrid System")
    st.caption("Cloud Edition â˜ï¸")
    st.text(f"Ver: {APP_VERSION}") # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å¸¸ã«è¡¨ç¤º

    # --- Live Progress Indicator ---
    from src.core.completion_checker import CompletionChecker
    
    current_plan_obj = st.session_state.get("current_plan")
    if current_plan_obj:
        try:
             checker = CompletionChecker(current_plan_obj)
             # Basic Info is Step 1, Goals Step 2... Let's use overall completeness
             missing_count = len(checker.check_missing_fields())
             total_fields = 20 # Estimate
             progress = max(0, min(100, int((20 - missing_count) / 20 * 100)))
             
             st.divider()
             st.progress(progress / 100)
             st.caption(f"ç¾åœ¨ã®é€²æ—: {progress}% (æ®‹ã‚Šé …ç›®: {missing_count})")
             
             if st.button("ğŸ“Š é€²æ—è©³ç´°ã‚’ç¢ºèª (Dashboard)", key="sidebar_progress_btn"):
                 st.session_state.app_nav_selection = "Dashboard Mode (Progress)"
                 st.rerun()
             st.divider()
        except:
             pass
    
    # Navigation Selection
    if "app_nav_selection" not in st.session_state:
        st.session_state.app_nav_selection = "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼"

    # Determine current index for radio
    interview_options = ["çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", "å¾“æ¥­å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", "å•†å·¥ä¼šè·å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼"]
    current_nav = st.session_state.app_nav_selection
    
    radio_index = 0
    if current_nav in interview_options:
        radio_index = interview_options.index(current_nav)
        
    # Callback to update state from radio
    def on_radio_change():
        st.session_state.app_nav_selection = st.session_state.nav_radio_key

    selected_interview = st.radio(
        "ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼é¸æŠ",
        interview_options,
        index=radio_index,
        key="nav_radio_key",
        on_change=on_radio_change
    )
    
    # Logic derivation (Internal)
    nav = st.session_state.app_nav_selection
    
    # --- State Tracking for Navigation Flux (Dashboard Return) ---
    if "last_chat_nav" not in st.session_state:
        st.session_state.last_chat_nav = "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼"

    valid_return_targets = [
        "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", 
        "å¾“æ¥­å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", 
        "å•†å·¥ä¼šè·å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", 
        "Main Consensus Room (å…¨ä½“åˆæ„)"
    ]
    if nav in valid_return_targets:
        st.session_state.last_chat_nav = nav
    # -------------------------------------------------------------

    if nav == "çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼":
        mode = "Chat Mode (Interview)"
        persona = "çµŒå–¶è€…"
    elif nav == "å¾“æ¥­å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼":
        mode = "Chat Mode (Interview)"
        persona = "å¾“æ¥­å“¡"
    elif nav == "å•†å·¥ä¼šè·å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼":
        mode = "Chat Mode (Interview)"
        persona = "å•†å·¥ä¼šè·å“¡"
    elif nav == "Main Consensus Room (å…¨ä½“åˆæ„)":
        mode = "Main Consensus Room (Resolution)"
        persona = "ç·åˆèª¿æ•´å½¹"
    elif nav == "Dashboard Mode (Progress)":
        mode = "Dashboard Mode (Progress)"
        persona = "Viewer"
    else: # Fallback
        mode = "Chat Mode (Interview)"
        persona = "çµŒå–¶è€…"

    # Manager Menu (Hidden by default)
    with st.expander("ç®¡ç†è€…ãƒ¡ãƒ‹ãƒ¥ãƒ¼", expanded=False):

        if st.button("å…¨ä½“åˆæ„ãƒ«ãƒ¼ãƒ  (Consensus)", use_container_width=True):
             st.session_state.app_nav_selection = "Main Consensus Room (å…¨ä½“åˆæ„)"
             st.rerun()
             
        if st.button("ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ (Progress)", use_container_width=True):
             st.session_state.app_nav_selection = "Dashboard Mode (Progress)"
             st.rerun()

        st.divider()
        st.caption("Data Management")
        
        # --- Upload (Import) ---
        import_owner_label = "ãƒ‡ãƒ¼ã‚¿æ‰€æœ‰è€… (ã‚¿ã‚°è£œå®Œç”¨)"
        import_owner = st.selectbox(
            import_owner_label, 
            ["è‡ªå‹• (Auto)", "çµŒå–¶è€…", "å¾“æ¥­å“¡", "å•†å·¥ä¼šè·å“¡"], 
            index=0,
            key="import_owner_select", # Changed key to avoid duplicate error if previously rendered? No, component moved.
            help="å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€éš›ã€èª°ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‹æŒ‡å®šã—ã¾ã™ã€‚ã€Œè‡ªå‹•ã€ã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®æƒ…å ±ã‚’å„ªå…ˆã—ã¾ã™ã€‚"
        )
        
        uploaded_file = st.file_uploader("JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—", type=["json"])
        
        if uploaded_file:
            file_id = f"{uploaded_file.name}_{uploaded_file.size}"
            if st.session_state.get("last_loaded_file_id") != file_id:
                try:
                    uploaded_file.seek(0)
                    data = json.load(uploaded_file)
                    
                    # Handle if data is a list (e.g. raw history list or basic_scenario)
                    if isinstance(data, list):
                        # Check if it looks like a valid history list (items must be dicts with 'role' and 'content')
                        if all(isinstance(m, dict) and "role" in m and "content" in m for m in data):
                             valid_history = data
                             if import_owner != "è‡ªå‹• (Auto)":
                                for msg in valid_history:
                                    if "persona" not in msg: msg["persona"] = import_owner
                                    if "target_persona" not in msg and msg.get("role") == "model": msg["target_persona"] = import_owner
                             
                             st.session_state.ai_interviewer.load_history(valid_history, merge=True)
                             st.session_state.loaded_msg_count = len(st.session_state.ai_interviewer.history)
                             st.toast(f"âœ… ä¼šè©±å±¥æ­´(ãƒªã‚¹ãƒˆå½¢å¼)ã‚’çµ±åˆã—ã¾ã—ãŸ ({len(valid_history)}ä»¶)", icon="ğŸ“¥")
                        else:
                             st.warning("âš ï¸ èª­ã¿è¾¼ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ (å¯¾å¿œå½¢å¼: äº‹æ¥­è¨ˆç”»JSON, ã¾ãŸã¯ä¼šè©±å±¥æ­´ãƒªã‚¹ãƒˆ)")
                    
                    elif isinstance(data, dict):
                        # 1. History Loading Logic (Existing Wrapper)
                        if "history" in data:
                            valid_history = [m for m in data.get("history", []) if isinstance(m, dict)]
                            
                            if import_owner != "è‡ªå‹• (Auto)":
                                for msg in valid_history:
                                    if "persona" not in msg and msg.get("role") == "user":
                                        msg["persona"] = import_owner
                                    if "target_persona" not in msg and msg.get("role") == "model":
                                        msg["target_persona"] = import_owner
                            
                            st.session_state.ai_interviewer.load_history(valid_history, merge=True)
                            st.session_state.loaded_msg_count = len(st.session_state.ai_interviewer.history)
                            st.toast(f"âœ… ä¼šè©±å±¥æ­´ã‚’çµ±åˆã—ã¾ã—ãŸ ({len(valid_history)}ä»¶)", icon="ğŸ“¥")

                        # 2. Direct Plan Loading Logic (New for Test Data)
                        # Check if 'basic_info' or 'goals' (key fields of ApplicationRoot) exists
                        elif "basic_info" in data or "goals" in data:
                            from src.api.schemas import ApplicationRoot
                            try:
                                # Migration Step
                                clean_data = ApplicationRoot.migrate_legacy_data(data)
                                
                                # Attempt to validate and load as current plan
                                plan = ApplicationRoot.model_validate(clean_data)
                                st.session_state.current_plan = plan
                                st.toast("âœ… äº‹æ¥­è¨ˆç”»ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ (Direct Load)", icon="ğŸ“„")

                                # --- Context Injection for Multi-Disaster Support ---
                                # Load the plan into history so the AI knows the baseline for subsequent discussions (e.g. Tsunami)
                                context_content = f"""
ã€ã‚·ã‚¹ãƒ†ãƒ é€šçŸ¥: æ—¢å­˜è¨ˆç”»ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã€‘
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä»¥ä¸‹ã®äº‹æ¥­è¨ˆç”»æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚
ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã€Œç¾åœ¨ã®æ±ºå®šäº‹é …ã€ã¨ã—ã¦èªè­˜ã—ã€ä»Šå¾Œã®ä¼šè©±ï¼ˆè¿½åŠ ã®ç½å®³å¯¾ç­–ãªã©ï¼‰ã¨çµ±åˆã—ã¦ãã ã•ã„ã€‚

```json
{json.dumps(clean_data, ensure_ascii=False, indent=2)}
```
"""
                                st.session_state.ai_interviewer.history.append({
                                    "role": "model", 
                                    "content": context_content,
                                    "persona": "AI Concierge",
                                    "target_persona": "General" # Visible to all
                                })
                            except Exception as val_e:
                                st.error(f"ãƒ‡ãƒ¼ã‚¿æ§‹é€ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {val_e}")
                                # Stop execution so user sees the error
                                st.stop()


                        
                        else:
                            st.warning("âš ï¸ èª­ã¿è¾¼ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿å½¢å¼ã§ã¯ã‚ã‚Šã¾ã›ã‚“ (history, basic_info, goals ã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“)")
                    else:
                         st.warning("âš ï¸ JSONå½¢å¼ãŒç„¡åŠ¹ã§ã™")

                    st.session_state.last_loaded_file_id = file_id
                    
                    # Auto-Redirect to Dashboard if Plan Loaded
                    if "current_plan" in st.session_state and st.session_state.current_plan:
                        st.session_state.app_nav_selection = "Dashboard Mode (Progress)"
                        
                        # Set Flash Message instead of immediate toast + sleep
                        company_name = st.session_state.current_plan.basic_info.corporate_name or "æœªè¨­å®š"
                        # Multi-line flash message via split toast
                        msg = f"âœ… äº‹æ¥­è¨ˆç”»ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ (Plan: {company_name})|||ğŸš€ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã¸ç§»å‹•ã—ã¾ã™"
                        set_flash_message(msg, icon="â¡ï¸")
                        
                    else:
                        st.toast("DEBUG: No Plan Loaded", icon="ğŸ›")
                        time.sleep(2) # Keep debug visible
                        
                    st.rerun()

                except Exception as e:
                    st.error(f"Error loading JSON: {e}")

        # --- Download (Export) ---
        if st.session_state.ai_interviewer.history:
            # 1. Full Backup
            full_history_json = json.dumps({"history": st.session_state.ai_interviewer.history}, indent=2, ensure_ascii=False)
            st.download_button(
                label="ğŸ“¦ å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ (Backup All)",
                data=full_history_json,
                file_name=f"jigyokei_full_backup_{int(time.time())}.json",
                mime="application/json"
            )

            # 1.5 Draft Plan Export (Markdown) - Only if analyzed
            # TEMPORARY: Disabled Markdown Export due to Schema Migration (JigyokeiPlan -> ApplicationRoot)
            if False and "current_plan" in st.session_state and st.session_state.current_plan:
                plan_export = st.session_state.current_plan
                # Simple MD generation
                md_text = f"# äº‹æ¥­ç¶™ç¶šåŠ›å¼·åŒ–è¨ˆç”»ï¼ˆä¸‹æ›¸ãï¼‰\\n\\n"
                md_text += f"## åŸºæœ¬æƒ…å ±\\n- ä¼æ¥­å: {plan_export.basic_info.company_name}\\n- ä»£è¡¨è€…: {plan_export.basic_info.representative_name}\\n- ä½æ‰€: {plan_export.basic_info.address}\\n\\n"
                md_text += f"## äº‹æ¥­å†…å®¹\\n- é¡§å®¢: {plan_export.business_content.target_customers}\\n- å•†å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹: {plan_export.business_content.products_services}\\n- æä¾›æ–¹æ³•: {plan_export.business_content.delivery_methods}\\n- å¼·ã¿: {plan_export.business_content.core_competence}\\n\\n"
                md_text += f"## è¢«å®³æƒ³å®š (ãƒªã‚¹ã‚¯)\\n"
                for r in plan_export.disaster_risks:
                    md_text += f"- {r.risk_type}: {r.impact_description}\\n"
                md_text += f"\\n## äº‹å‰å¯¾ç­–\\n"
                for m in plan_export.pre_disaster_measures:
                    md_text += f"- {m.item}: {m.content} (æ‹…å½“: {m.in_charge})\\n"
                
                st.download_button(
                    label="ğŸ“ ä¸‹æ›¸ãã‚·ãƒ¼ãƒˆã‚’ä¿å­˜ (Markdown)",
                    data=md_text,
                    file_name=f"jigyokei_draft_{int(time.time())}.md",
                    mime="text/markdown",
                    help="è§£ææ¸ˆã¿ã®è¨ˆç”»æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚"
                )
            
            # 2. Persona Specific Export
            # Note: Need to access 'persona' variable which is derived LATER.
            # CRITICAL: We cannot access 'persona' here because it is defined AFTER this block.
            # Implication: We should calculate 'persona' inside the expander OR rely on session state if available.
            # BUT 'persona' depends on 'nav' which IS available (st.session_state.app_nav_selection).
            # Let's verify 'nav' logic.
             
             # --- Sidebar: Recommended Actions (Mirrors Dashboard) ---
    with st.sidebar:
        st.divider()
        st.subheader("ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (Recommended)")
        
        # 1. From Consensus Chat Suggestions
        if "_consensus_suggestions" in st.session_state:
            sugg = st.session_state._consensus_suggestions
            if "options" in sugg and sugg["options"]:
                st.caption("ğŸ¤– AIã‹ã‚‰ã®ææ¡ˆ:")
                for opt in sugg["options"]:
                    st.info(f"ğŸ‘‰ {opt}")
        
        # 2. From Missing Items (Static Analysis)
        if "current_plan" in st.session_state and st.session_state.current_plan:
             from src.core.completion_checker import CompletionChecker
             # Fix: Use static method directly
             result = CompletionChecker.analyze(st.session_state.current_plan)
             if result["missing_mandatory"]:
                 st.caption("âš ï¸ æœªå®šã®å¿…é ˆé …ç›®:")
                 for m in result["missing_mandatory"][:3]: # Show top 3
                     st.warning(f"ğŸ“Œ {m['section']}")
        else:
            st.caption("â„¹ï¸ è¨ˆç”»ãƒ‡ãƒ¼ã‚¿æœªèª­ã¿è¾¼ã¿")

    # --- Main Area ---


# --- Main Area ---

# Check Flash Messages at Start of Render Cycle
check_flash_message()

if mode == "Chat Mode (Interview)":
    # --- Ensure Initial State for Optimization (Empty Plan + Dashboard) ---
    if "current_plan" not in st.session_state:
        from src.api.schemas import ApplicationRoot
        # Initialize blank plan object (not dict) for CompletionChecker compatibility
        st.session_state.current_plan = ApplicationRoot()
    
    # 1. Dashboard Navigation & Header
    st.title("ğŸ¤– AI Interviewer (Chat Mode)")

    # User Metadata Inputs (Main Panel) - Always visible at top
    with st.container(border=True):
        st.caption(f"ğŸ“ {persona}æƒ…å ±å…¥åŠ›")
        col_u1, col_u2 = st.columns(2)
        with col_u1:
             pos_placeholder = "ä¾‹: ä»£è¡¨å–ç· å½¹"
             if persona == "å¾“æ¥­å“¡": pos_placeholder = "ä¾‹: ç¾å ´ç›£ç£"
             elif persona == "å•†å·¥ä¼šè·å“¡": pos_placeholder = "ä¾‹: çµŒå–¶æŒ‡å°å“¡"
             st.text_input("å½¹è· (Position)", key="user_position_input", placeholder=pos_placeholder)
        with col_u2:
             st.text_input("ãŠåå‰ (Name)", key="user_name_input", placeholder="ä¾‹: å±±ç”° å¤ªéƒ")

    # 2. Document Upload Area (Always Available)
    with st.expander("ğŸ“‚ è³‡æ–™ã®è¿½åŠ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (Upload Documents)", expanded=not st.session_state.ai_interviewer.history):
        # Persona-specific Guidance
        upload_label = "è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF/ç”»åƒ)"
        if persona == "çµŒå–¶è€…":
            st.info("ğŸ¢ **çµŒå–¶è€…ã®æ–¹ã¸**: ä¼šç¤¾æ¡ˆå†…ã€äº‹æ¥­è¨ˆç”»æ›¸ã€ãƒã‚¶ãƒ¼ãƒ‰ãƒãƒƒãƒ—ãªã©")
            upload_label = "ğŸ¢ çµŒå–¶è€…ç”¨è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
        elif persona == "å¾“æ¥­å“¡":
            st.info("ğŸ‘· **å¾“æ¥­å“¡ã®æ–¹ã¸**: æ¥­å‹™ãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã€ç·Šæ€¥é€£çµ¡ç¶²ã€ç¾å ´å†™çœŸãªã©")
            upload_label = "ğŸ‘· ç¾å ´ãƒ»æ¥­å‹™è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
        elif persona == "å•†å·¥ä¼šè·å“¡":
            st.info("ğŸ§‘â€ğŸ« **å•†å·¥ä¼šè·å“¡ã®æ–¹ã¸**: å…±æ¸ˆãƒ‘ãƒ³ãƒ•ãƒ¬ãƒƒãƒˆã€åœ°åŸŸé˜²ç½è¨ˆç”»ãªã©")
            upload_label = "ğŸ§‘â€ğŸ« æ”¯æ´ãƒ»åˆ¶åº¦è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"
        
        uploaded_refs = st.file_uploader(
            upload_label, 
            type=["pdf", "png", "jpg", "jpeg"], 
            accept_multiple_files=True,
            key=f"uploader_{persona}" # Remove timestamp to keep uploader stable
        )
        
        # --- Auto-Process Logic ---
        if "processed_file_ids" not in st.session_state:
            st.session_state.processed_file_ids = set()

        if uploaded_refs:
            new_files_to_process = []
            for file in uploaded_refs:
                # Create a simple unique ID for the file instance
                file_id = f"{file.name}_{file.size}"
                if file_id not in st.session_state.processed_file_ids:
                    new_files_to_process.append(file)
                    st.session_state.processed_file_ids.add(file_id)
            
                if new_files_to_process:
                # Automatically process new files
                 with st.spinner("è³‡æ–™ã‚’è§£æä¸­... (Auto-Processing)"):
                    try:
                        count = st.session_state.ai_interviewer.process_files(new_files_to_process, target_persona=persona)
                        st.success(f"{count}ä»¶ã®æ–°ã—ã„è³‡æ–™ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼")
                        
                        # --- Agentic Extraction Trigger (File Upload) ---
                        if count > 0:
                            with st.status("ğŸ¤– AI Agent Working: è³‡æ–™ã‚’è©³ç´°åˆ†æä¸­...", expanded=True) as status:
                                 status.write("ğŸ“ Gemini Experimental (High Reasoning Preview) ã§è³‡æ–™ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
                                 try:
                                     all_files = st.session_state.ai_interviewer.uploaded_file_refs
                                     extracted_data = st.session_state.ai_interviewer.extract_structured_data(text="", file_refs=all_files)
                                     
                                     if extracted_data:
                                         status.write("âœ… æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚è¨ˆç”»æ›¸ã«åæ˜ ã—ã¾ã™...")
                                         
                                         # Merge Logic
                                         try:
                                             # Convert to Schema if not already (assuming dict return)
                                             from src.api.schemas import ApplicationRoot
                                             
                                             # Initialize plan if None
                                             if not st.session_state.get("current_plan"):
                                                  st.session_state.current_plan = ApplicationRoot() # Empty Init
                                             
                                             # Update fields (Recursive merge or Pydantic copy?)
                                             # Ideally we use a merge utility, but for now we re-validate the merged dict.
                                             current_dict = st.session_state.current_plan.model_dump()
                                             
                                             # Simple recursive merge helper
                                             def deep_merge(base, update):
                                                 for k, v in update.items():
                                                     if isinstance(v, dict) and k in base and isinstance(base[k], dict):
                                                         deep_merge(base[k], v)
                                                     elif v is not None: # Only overwrite if not None
                                                         base[k] = v
                                                 return base
                                             
                                             merged_dict = deep_merge(current_dict, extracted_data)
                                             st.session_state.current_plan = ApplicationRoot.model_validate(merged_dict)
                                             
                                             status.write("ğŸ’¡ èª­ã¿è¾¼ã‚“ã æƒ…å ±ã‚’è¨ˆç”»æ›¸ã«çµ±åˆã—ã¾ã—ãŸã€‚")
                                             
                                             # Add Context to History
                                             context_msg = f"ã€ã‚·ã‚¹ãƒ†ãƒ é€šçŸ¥: è‡ªå‹•æŠ½å‡ºå®Œäº†ã€‘\nè³‡æ–™ã‹ã‚‰ä»¥ä¸‹ã®æƒ…å ±ã‚’æŠ½å‡ºã—ã€è¨ˆç”»æ›¸ã«åæ˜ ã—ã¾ã—ãŸã€‚\n{json.dumps(extracted_data, ensure_ascii=False, indent=2)}"
                                             st.session_state.ai_interviewer.history.append({
                                                "role": "model", 
                                                "content": context_msg,
                                                "persona": "AI Concierge",
                                                "target_persona": "General"
                                             })
                                             
                                         except Exception as merge_e:
                                             status.error(f"Merge Error: {merge_e}")

                                     else:
                                         status.write("â„¹ï¸ æ–°è¦ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                                 except Exception as ex_e:
                                     status.error(f"Extraction Error: {ex_e}")
                        
                                     status.error(f"Extraction Error: {ex_e}")
                        
                        time.sleep(5)
                        # Inline Auto-Save (Fix NameError)
                        if "session_manager" in st.session_state:
                             p_data = st.session_state.current_plan.model_dump() if st.session_state.get("current_plan") else None
                             st.session_state.session_manager.save_session(
                                 history=st.session_state.ai_interviewer.history,
                                 current_plan_dict=p_data,
                                 session_id="mobile_autosave"
                             )
                        st.rerun()
                    except Exception as e:
                        st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    # 3. Chat Interface
    st.divider()
    
    # History Display
    if not st.session_state.ai_interviewer.history:
        st.markdown(
            "ğŸ‘‹ **ã“ã‚“ã«ã¡ã¯ã€‚äº‹æ¥­ç¶™ç¶šåŠ›å¼·åŒ–è¨ˆç”»ã®ç­–å®šã‚’æ”¯æ´ã—ã¾ã™ã€‚**\n\n"
            "ã¾ãšã¯ä¸Šã®ã€Œè³‡æ–™ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã‹ã‚‰è³‡æ–™ã‚’èª­ã¿è¾¼ã¾ã›ã‚‹ã‹ã€"
            "ä¸‹ã®å…¥åŠ›æ¬„ã‹ã‚‰ä¼šè©±ã‚’å§‹ã‚ã¦ãã ã•ã„ã€‚"
        )
    
    # Helper to render a single message
    def render_message(msg, current_persona):
        if not isinstance(msg, dict): return
        role = msg["role"]
        msg_persona = msg.get("persona", "Unknown")
        target_persona = msg.get("target_persona")
        
        # Filtering Logic
        visible = False
        if role == "user" and msg_persona == current_persona:
            visible = True
        elif role == "model" and target_persona == current_persona:
            visible = True
        
        if visible:
            avatar = "ğŸ¤–" if role == "model" else "ğŸ‘¤"
            if msg_persona == "çµŒå–¶è€…": avatar = "ğŸ‘¨â€ğŸ’¼"
            elif msg_persona == "å¾“æ¥­å“¡": avatar = "ğŸ‘·"
            elif msg_persona == "å•†å·¥ä¼šè·å“¡": avatar = "ğŸ§‘â€ğŸ«"
            elif msg_persona == "AI Concierge": avatar = "ğŸ¤–"
            
            with st.chat_message(role, avatar=avatar):
                if role == "user":
                    st.caption(f"{msg_persona}")
                
                # Sanitize content
                # Sanitize content
                import re
                
                # Check for <data> block (Final Output)
                data_match = re.search(r'<data>(.*?)</data>', msg["content"], flags=re.DOTALL)
                
                if data_match:
                    # Hide the raw data from display
                    display_content = re.sub(r'<data>.*?</data>', '', msg["content"], flags=re.DOTALL).strip()
                    st.markdown(display_content)
                    
                    # Show "Check Progress" button
                    st.button(
                        "ğŸ“Š ãƒ’ã‚¢ãƒªãƒ³ã‚°å®Œäº†: é€²æ—ã‚’ç¢ºèªã™ã‚‹ (Check Progress)", 
                        key=f"btn_complete_{len(msg['content'])}", 
                        type="primary",
                        on_click=auto_complete_interview,
                        args=(data_match.group(1).strip(),)
                    )
                else:
                    # Standard display (Hide suggestions and updates)
                    display_content = re.sub(r'<suggestions>.*?</suggestions>', '', msg["content"], flags=re.DOTALL)
                    display_content = re.sub(r'<update>.*?</update>', '', display_content, flags=re.DOTALL).strip()
                    st.markdown(display_content)

                # Capture suggestions (only from model)
                if role == "model":
                    match = re.search(r'<suggestions>(.*?)</suggestions>', msg["content"], flags=re.DOTALL)
                    if match:
                        try:
                            st.session_state._temp_suggestions = json.loads(match.group(1))
                        except:
                            pass
                            pass
    # Reset temp suggestions
    if "_temp_suggestions" in st.session_state:
        del st.session_state["_temp_suggestions"]

    # --- Auto-Scroll Logic ---
    # [DISABLED] User requested to prevent auto-scrolling to verify AI response content comfortably.
    # The previous JS injection has been removed.
    
    # Ensure baseline is set if it's the first run or reset
    current_len = len(st.session_state.ai_interviewer.history)
    if "last_history_len" not in st.session_state:
        st.session_state["last_history_len"] = current_len

    history = st.session_state.ai_interviewer.history
    loaded_count = st.session_state.get("loaded_msg_count", 0)



    # 1. Past History (Collapsible)
    if loaded_count > 0:
        with st.expander("ğŸ•’ éå»ã®ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤º (Loaded History)", expanded=False):
            for i in range(loaded_count):
                if i < len(history):
                     render_message(history[i], persona)

    # 2. Main Chat Area Container (To ensure new messages appear above the dashboard)
    main_chat_container = st.container()
    
    with main_chat_container:
        # New Session History
        for i in range(loaded_count, len(history)):
            render_message(history[i], persona)

    # --- Rendering Contextual Support (Hints & Examples) ---
    # Retrieve suggestions from LAST message if it was from model
    last_msg = history[-1] if history else None
    current_suggestions = {}
    
    if last_msg and last_msg["role"] == "model":
        import re
        match = re.search(r'<suggestions>(.*?)</suggestions>', last_msg["content"], flags=re.DOTALL)
        if match:
            try:
                current_suggestions = json.loads(match.group(1))
                # Persist valid suggestions
                st.session_state.last_valid_suggestions = current_suggestions
            except:
                pass
    
    # Fallback to last valid suggestions if current parsing failed (to maintain Optimized Layout)
    if not current_suggestions and "last_valid_suggestions" in st.session_state:
        current_suggestions = st.session_state.last_valid_suggestions

    suggested_prompt = None

    # --- Render Advice in Placeholder (In-place Update) ---
    advice_placeholder = st.empty()

    def render_advice_in_placeholder(placeholder, suggestions):
        """Renders the AI hints and example box inside a placeholder."""
        if not suggestions:
            placeholder.empty()
            return

        hints = suggestions.get("hints")
        example = suggestions.get("example")
        
        if hints or example:
            with placeholder.container():
                with st.container(border=True): # Distinct box for AI assistance
                    st.caption("ğŸ’¡ AIã‹ã‚‰ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹")
                    if hints:
                        st.info(f"**ãƒ’ãƒ³ãƒˆ**: {hints}")
                    if example:
                        st.success(f"**å›ç­”ä¾‹**: {example}")
                        # Improvement: Button to use the example as answer
                        # Use stable key based on content hash AND history length to ensure uniqueness per turn
                        import hashlib
                        # Include length of history to differentiate "Yes" at turn 1 vs "Yes" at turn 5
                        unique_str = f"{example}_{len(st.session_state.ai_interviewer.history)}"
                        stable_key = hashlib.md5(unique_str.encode()).hexdigest()
                        
                        # Note: Buttons inside placeholders might have issues if not handled carefully during rerun?
                        # Actually if we don't rerun, the button callback won't trigger standard rerun?
                        # Wait, button click triggers rerun. If we don't rerun here, the button appears. 
                        # Clicking it triggers rerun -> script runs -> placeholder re-renders.
                        # It should work.
                        if st.button("ğŸ“‹ å›ç­”ä¾‹ã®é€šã‚Šå›ç­”ã™ã‚‹", key=f"use_example_{stable_key}"):
                            # Setting session state for prompt pre-fill?
                            # prompt = st.chat_input... can't be pre-filled easily without key manipulation.
                            # Standard pattern: specific variable
                            # But st.chat_input doesn't support 'value'.
                            # Workaround: We can't easily prefill chat_input.
                            # Solution: We treat clicking the button AS SENDING the message?
                            # "å›ç­”ä¾‹ã®é€šã‚Šå›ç­”ã™ã‚‹" -> Submit immediately. 
                            st.session_state.auto_trigger_message = example
                            st.rerun()
                            # return example
        return None

    # Initial Render
    clicked_example = render_advice_in_placeholder(advice_placeholder, current_suggestions)
    if clicked_example:
        suggested_prompt = clicked_example

    # --- Next Action Suggestions (Quick Replies) ---
    # Prioritize dynamic options
    options = current_suggestions.get("options")
    if not options: # Handle None or Empty list
        options = []
    
    # Fallback if no dynamic options (Double check to ensure buttons always appear)
    if not options:
        # --- Context-Aware Dynamic Fallback ---
        # Analyze the last message content to provide relevant options
        last_content = last_msg["content"] if last_msg else ""
        
        context_options = []
        if "å½¹è·" in last_content:
            context_options = ["ä»£è¡¨å–ç· å½¹", "åº—é•·", "å·¥å ´é•·", "ç¤¾å“¡"]
        elif "åå‰" in last_content:
            context_options = ["ç¢ºèªã—ã¦å…¥åŠ›"] # 'Same as above' is often confusing if nothing above
        elif "é¿é›£" in last_content:
             context_options = ["æŒ‡å®šé¿é›£æ‰€ã¸å¾’æ­©ã§", "é«˜å°ã¸è»Šã§", "ç¤¾å±‹ã®2éšã¸å‚ç›´é¿é›£", "è‡ªå®…å¾…æ©Ÿ"]
        elif "å®‰å¦" in last_content:
             context_options = ["å®‰å¦ç¢ºèªã‚·ã‚¹ãƒ†ãƒ ", "LINEã‚°ãƒ«ãƒ¼ãƒ—", "é›»è©±é€£çµ¡", "ä¸€æ–‰ãƒ¡ãƒ¼ãƒ«"]
        elif "è¢«å®³" in last_content and ("æƒ³å®š" in last_content or "å½±éŸ¿" in last_content):
             context_options = ["æµ¸æ°´è¢«å®³", "å»ºç‰©ã®å€’å£Š", "åœé›»ãƒ»æ–­æ°´", "ç‰©æµã®åœæ­¢"]
        
        if context_options:
            options = context_options
        else:
            # Standard Fallback if no context detected
            fallback_map = {
                "çµŒå–¶è€…": ["äº‹æ¥­ã®å¼·ã¿ã«ã¤ã„ã¦", "è‡ªç„¶ç½å®³ã¸ã®æ‡¸å¿µ", "é‡è¦ãªè¨­å‚™ãƒ»è³‡ç”£"],
                "å¾“æ¥­å“¡": ["ç·Šæ€¥æ™‚ã®é€£çµ¡ä½“åˆ¶", "é¿é›£çµŒè·¯ã®ç¢ºèª", "é¡§å®¢å¯¾å¿œãƒãƒ‹ãƒ¥ã‚¢ãƒ«"],
                "å•†å·¥ä¼šè·å“¡": ["ãƒã‚¶ãƒ¼ãƒ‰ãƒãƒƒãƒ—ç¢ºèª", "æå®³ä¿é™ºã®åŠ å…¥çŠ¶æ³", "åœ°åŸŸé˜²ç½è¨ˆç”»ã¨ã®é€£æº"]
            }
            # Default to "çµŒå–¶è€…" if persona key missing
            options = fallback_map.get(persona, fallback_map["çµŒå–¶è€…"])
        
        # Inject standard options into current suggestion to persist them
        if not current_suggestions:
            current_suggestions = {"options": options}
            st.session_state.last_valid_suggestions = current_suggestions

    # --- Options Placeholder (After Advice) ---
    options_placeholder = st.empty()

    def render_options_in_placeholder(placeholder, current_options):
        with placeholder.container():
             # Remove duplicate caption if it exists outside
             if current_options:
                 st.caption("ğŸ‘‡ ã‚¯ã‚¤ãƒƒã‚¯è¿”ä¿¡ (ã‚¯ãƒªãƒƒã‚¯ã§é€ä¿¡)")
                 # Ensure horizontal layout - one row
                 cols = st.columns(len(current_options))
                 for idx, opt in enumerate(current_options):
                     with cols[idx]:
                         # Use strict key
                         if st.button(opt, key=f"opt_{idx}_{len(st.session_state.ai_interviewer.history)}", use_container_width=True):
                             st.session_state.auto_trigger_message = opt
                             st.rerun()

    # Render Options
    render_options_in_placeholder(options_placeholder, options)

    # --- Mini Progress Dashboard (Placeholder) - MOVED TO BOTTOM ---
    dashboard_placeholder = st.empty()

    def render_mini_dashboard_in_placeholder(placeholder):
        # Ensure it renders something even if plan is missing (for debugging/fallback)
        with placeholder.container():
            if "current_plan" in st.session_state and st.session_state.current_plan:
                from src.core.completion_checker import CompletionChecker
                res = CompletionChecker.analyze(st.session_state.current_plan)
                prog = res['mandatory_progress']
                
                # 1. Next Actions (First)
                if res['missing_mandatory']:
                    sec_map = {"BasicInfo": "åŸºæœ¬æƒ…å ±", "Goals": "äº‹æ¥­æ¦‚è¦", "Disaster": "ç½å®³æƒ³å®š", "ResponseProcedures": "åˆå‹•å¯¾å¿œ", "Measures": "äº‹å‰å¯¾ç­–", "FinancialPlan": "è³‡é‡‘è¨ˆç”»", "PDCA": "æ¨é€²ä½“åˆ¶"}
                    next_items = [sec_map.get(m['section'], m['section']) for m in res['missing_mandatory'][:3]]
                    
                    # Interactive Next Actions
                    st.caption("ğŸ“Œ **æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (ã‚¯ãƒªãƒƒã‚¯ã§å…¥åŠ›ã‚’é–‹å§‹):**")
                    cols_next = st.columns(len(next_items))
                    for idx, item in enumerate(next_items):
                        with cols_next[idx]:
                            # Use dynamic key based on history to avoid duplicates
                            hist_len_act = len(st.session_state.ai_interviewer.history) if "ai_interviewer" in st.session_state else 0
                            if st.button(f"ğŸ“ {item}", key=f"next_act_{idx}_{hist_len_act}", use_container_width=True):
                                st.session_state.auto_trigger_message = f"{item}ã®å…¥åŠ›ã‚’è¡Œã„ãŸã„ã§ã™ã€‚ä½•ã‹ã‚‰å§‹ã‚ã‚Œã°ã‚ˆã„ã§ã™ã‹ï¼Ÿ"
                                st.rerun()
                
                # 2. Progress Bar (Second)
                cols_prog = st.columns([3, 1, 1.5]) # Added column for button
                with cols_prog[0]: st.progress(prog)
                with cols_prog[1]: st.caption(f"**{int(prog*100)}% å®Œäº†**")
                with cols_prog[2]:
                    # Use dynamic key to prevent StreamlitDuplicateElementKey when re-rendering in the same run
                    # (Initial Render + In-place Update)
                    hist_len = len(st.session_state.ai_interviewer.history) if "ai_interviewer" in st.session_state else 0
                    if st.button("ğŸ“Š è©³ç´°ã‚’ç¢ºèª", key=f"btn_check_prog_dash_{hist_len}", use_container_width=True):
                         change_mode("Dashboard Mode (Progress)")
                         st.rerun()
            else:
                # Debug feedback if plan is missing
                # st.warning("âš  äº‹æ¥­è¨ˆç”»ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                pass 

    # Render Dash Initial
    render_mini_dashboard_in_placeholder(dashboard_placeholder)

    # Input Area



    # Input Area
    prompt = st.chat_input(f"{persona}ã¨ã—ã¦å›ç­”ã‚’å…¥åŠ›...", key="chat_input_main")

    if st.session_state.get("auto_trigger_message"):
        prompt = st.session_state.auto_trigger_message
        st.session_state.auto_trigger_message = None

    if prompt:
        with main_chat_container:
            with st.chat_message("user", avatar="ğŸ§‘â€ğŸ«" if persona=="å•†å·¥ä¼šè·å“¡" else "ğŸ‘¤"):
                st.markdown(prompt)
        
        final_prompt = prompt
        
        # Prepare metadata for context
        user_name = st.session_state.get("user_name_input", "")
        user_position = st.session_state.get("user_position_input", "")
        user_data = {"name": user_name, "position": user_position}

        # --- Agentic Smart Extraction (Experimental) ---
        if len(final_prompt) > 200 or "è³‡æ–™" in final_prompt:
             with st.status("ğŸ¤– AI Agent Working: æƒ…å ±ã‚’æŠ½å‡ºä¸­...", expanded=False) as status:
                  status.write("ğŸ“ æ–‡è„ˆã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿å–ã£ã¦ã„ã¾ã™ (Extracting Facts)...")
                  try:
                      extracted_data = st.session_state.ai_interviewer.extract_structured_data(final_prompt)
                      if extracted_data:
                          status.write("âœ… ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚è¨ˆç”»æ›¸ã«åæ˜ ã—ã¾ã™ã€‚")
                          pass # Logic handled inside AIInterviewer parsing for now
                  except Exception as e:
                      print(f"Extraction failed: {e}")
                      status.update(label="âš ï¸ Extraction skipped", state="error")
        
        # Determine who responds: Model or just UI update (Wait, logic flow check)
        # The structure here is: if we have a prompt (user input or suggestion), we send it.
        
        with main_chat_container:
            # Inject Anchor for Scroll (Target for Auto-Scroll)
            st.markdown('<div id="latest-response"></div>', unsafe_allow_html=True)
            
            with st.chat_message("model", avatar="ğŸ¤–"):
                with st.spinner("AI is thinking..."):
                    response = st.session_state.ai_interviewer.send_message(
                    final_prompt, 
                    persona=persona,
                    user_data=user_data
                )
                    
                    # Scroll to Anchor using JS (Wait for render > 500ms)
                    import streamlit.components.v1 as components
                    js_code = """
                        <script>
                        setTimeout(function() {
                            const element = window.parent.document.getElementById('latest-response');
                            if (element) {
                                element.scrollIntoView({behavior: 'smooth', block: 'start'});
                            }
                        }, 500);
                        </script>
                    """
                    components.html(js_code, height=0)
                # Sanitize content for display (Hide <suggestions> block implementation)
                import re
                
                # Extract and Apply <update> Incremental Data
                update_match = re.search(r'<update>(.*?)</update>', response, flags=re.DOTALL)
                if update_match:
                    try:
                        update_json_str = update_match.group(1).strip()
                        update_data = json.loads(update_json_str)
                        if apply_incremental_update(update_data):
                            print("Applied Incremental Update")
                            st.toast("âš¡ ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¾ã—ãŸ", icon="ğŸ“")
                    except Exception as e:
                        print(f"Update Parse Failed: {e}")
                
                 # Force Dashboard Update (In-place) after every response (to reflect new state/progress)
                render_mini_dashboard_in_placeholder(dashboard_placeholder)
                
                # Strip tags for display
                display_response = re.sub(r'<suggestions>.*?</suggestions>', '', response, flags=re.DOTALL)
                display_response = re.sub(r'<update>.*?</update>', '', display_response, flags=re.DOTALL).strip()
                
                st.markdown(display_response)

                # --- Auto-Save Hook ---
                perform_auto_save()
                
                # Update Options & Advice if suggestions found
                # Typically options update requires rerun because button keys must be unique or handled.
                # But we used length-based key. History length increased by 2 (User+AI).
                # So keys will be unique.
                # We DO NOT RERUN to prevent scroll.
                match_sugg = re.search(r'<suggestions>(.*?)</suggestions>', response, flags=re.DOTALL)
                if match_sugg:
                    try:
                        new_sugg = json.loads(match_sugg.group(1))
                        
                        # Update Advice & Options ONLY if history length changed (to avoid duplicate key error with Initial Render)
                        # The keys for buttons depend on history length. 
                        # If len hasn't changed (e.g. history update issues), rendering again crashes Streamlit.
                        new_hist_len = len(st.session_state.ai_interviewer.history)
                        if new_hist_len > current_len:
                            render_advice_in_placeholder(advice_placeholder, new_sugg)
                            
                            new_opts = new_sugg.get("options", [])
                            # Logic to fallback if options missing in update
                            if not new_opts:
                                fallback_map = {
                                    "çµŒå–¶è€…": ["äº‹æ¥­ã®å¼·ã¿ã«ã¤ã„ã¦", "è‡ªç„¶ç½å®³ã¸ã®æ‡¸å¿µ", "é‡è¦ãªè¨­å‚™ãƒ»è³‡ç”£"],
                                    "å¾“æ¥­å“¡": ["ç·Šæ€¥æ™‚ã®é€£çµ¡ä½“åˆ¶", "é¿é›£çµŒè·¯ã®ç¢ºèª", "é¡§å®¢å¯¾å¿œãƒãƒ‹ãƒ¥ã‚¢ãƒ«"],
                                    "å•†å·¥ä¼šè·å“¡": ["ãƒã‚¶ãƒ¼ãƒ‰ãƒãƒƒãƒ—ç¢ºèª", "æå®³ä¿é™ºã®åŠ å…¥çŠ¶æ³", "åœ°åŸŸé˜²ç½è¨ˆç”»ã¨ã®é€£æº"]
                                }
                                new_opts = fallback_map.get(persona, [])
                            
                            render_options_in_placeholder(options_placeholder, new_opts)
                        else:
                            print(f"Skipping placeholder update: History length {new_hist_len} == {current_len}")
                    except: pass
elif mode == "Dashboard Mode (Progress)":
    # Navigation Header for Dashboard
    col_dash_head1, col_dash_head2 = st.columns([3, 1])
    with col_dash_head1:
        st.title("ğŸ“Š é€²æ—ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰")
    with col_dash_head2:
        # 3-Way Back Navigation
        st.button("â¬…ï¸ çµŒå–¶è€…ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", on_click=change_mode, args=("Chat Mode (Interview)", "çµŒå–¶è€…"), use_container_width=True)
        st.button("â¬…ï¸ å¾“æ¥­å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", on_click=change_mode, args=("Chat Mode (Interview)", "å¾“æ¥­å“¡"), use_container_width=True)
        st.button("â¬…ï¸ å•†å·¥ä¼šè·å“¡ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼", on_click=change_mode, args=("Chat Mode (Interview)", "å•†å·¥ä¼šè·å“¡"), use_container_width=True)

    st.info("ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‹ã‚‰äº‹æ¥­è¨ˆç”»æ›¸ã®å®Œæˆåº¦ã‚’è‡ªå‹•åˆ¤å®šã—ã¾ã™ã€‚")
    
    from src.api.schemas import ApplicationRoot
    
    # Auto-Analysis / Display Logic (No Manual Button)
    if "current_plan" in st.session_state:
        plan: ApplicationRoot = st.session_state.current_plan
        from src.core.completion_checker import CompletionChecker
        
        # Run Analysis
        result = CompletionChecker.analyze(plan)
        
        # --- 1. Status Banner & Header ---
        st.divider()
        st.subheader("ğŸ“Š äº‹æ¥­è¨ˆç”»æ›¸ å®Œæˆåº¦è¨ºæ–­")
        
        col_m1, col_m2 = st.columns([1, 4])
        with col_m1:
            st.metric(label="èªå®šå¯èƒ½æ€§ã‚¹ã‚³ã‚¢", value=f"{result['total_score']} / 100", help="100ç‚¹ã§é›»å­ç”³è«‹ã®èªå®šè¦ä»¶ã‚’æº€ãŸã—ã¾ã™")
            
        with col_m2:
            st.caption("èªå®šã«å‘ã‘ãŸå¿…é ˆé …ç›®ã®å…¥åŠ›çŠ¶æ³")
            st.progress(result['mandatory_progress'])
            st.caption(f"å¿…é ˆé …ç›®ã®é”æˆç‡: {int(result['mandatory_progress']*100)}% å®Œäº†")
            
        # --- 2. Actionable Alerts (Missing Mandatory) - SEVERITY-BASED ---
        if result['status'] != "success":
            with st.container(border=True): # Red/Error container simulation
                st.error("ğŸš¨ ç”³è«‹ã«å‘ã‘ã¦ã€ä»¥ä¸‹ã®å¿…é ˆé …ç›®ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
                # Define a mapping for section names to Japanese
                section_map = {
                    "BasicInfo": "åŸºæœ¬æƒ…å ±",
                    "Goals": "äº‹æ¥­æ¦‚è¦ãƒ»ç›®æ¨™",
                    "ResponseProcedures": "åˆå‹•å¯¾å¿œ",
                    "Measures": "äº‹å‰å¯¾ç­–",
                    "FinancialPlan": "è³‡é‡‘è¨ˆç”»",
                    "PDCA": "æ¨é€²ä½“åˆ¶ (PDCA)"
                }
                
                # ... (Logic omitted for brevity in tool call, but context needs to match) ...
                # Group by severity for clearer display
                critical_items = [m for m in result['missing_mandatory'] if m.get('severity') == 'critical']
                warning_items = [m for m in result['missing_mandatory'] if m.get('severity') == 'warning']
                
                if critical_items:
                    st.markdown("### ğŸ”´ **æœªå…¥åŠ› (å¿…é ˆ)**")
                    for item in critical_items:
                        sec_label = section_map.get(item['section'], item['section'])
                        st.error(f"**{sec_label}**: {item['msg']}", icon="ğŸ”´")
                
                if warning_items:
                    st.markdown("### ğŸŸ¡ **å…¥åŠ›ä¸è¶³ (è¦ç¢ºèª)**")
                    for item in warning_items:
                        sec_label = section_map.get(item['section'], item['section'])
                        st.warning(f"**{sec_label}**: {item['msg']}", icon="ğŸŸ¡")
                
                with st.columns(2)[0]:
                    # Prepare args for callback
                    missing_msgs = [m['msg'] for m in result['missing_mandatory']]
                    
                    st.button(
                        "ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ã‚¢ãƒ¼ã«ä¸è¶³é …ç›®ã‚’èã„ã¦ã‚‚ã‚‰ã†", 
                        type="primary", 
                        key="btn_ask_missing",
                        on_click=on_click_ask_missing,
                        args=(missing_msgs,)
                    )

        elif result['recommended_progress'] < 1.0:
            st.success("âœ… ç”³è«‹è¦ä»¶ã¯ã‚¯ãƒªã‚¢ã—ã¦ã„ã¾ã™ï¼ (ã•ã‚‰ã«è¨ˆç”»ã‚’å¼·åŒ–ã—ã¾ã—ã‚‡ã†)")
            with st.expander("ğŸ’¡ ã•ã‚‰ãªã‚‹å“è³ªå‘ä¸Šã®ãƒ’ãƒ³ãƒˆ (Recommended Actions)", expanded=True):
                for sug in result['suggestions']:
                    st.info(f"Suggestion: {sug}")

        else:
             st.balloons()
             st.success("ğŸ† Perfect! è¨ˆç”»ã¯å®Œç’§ã§ã™ã€‚ç”³è«‹ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚")
        
        st.divider()
        col_exp1, col_exp2 = st.columns([3, 1])
        with col_exp2:
            # Excel Export
            if st.button("ğŸ“„ ä¸‹æ›¸ãã‚·ãƒ¼ãƒˆå‡ºåŠ› (Excel)", key="btn_export_draft", use_container_width=True):
                try:
                    from src.core.draft_exporter import DraftExporter
                    excel_data = DraftExporter.export_to_excel(plan, result)
                    st.download_button(
                        label="â¬‡ï¸ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹",
                        data=excel_data,
                        file_name=f"jigyokei_draft_{plan.basic_info.corporate_name or 'plan'}.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                        key="btn_download_excel_real"
                    )
                    st.success("Excelç”Ÿæˆå®Œäº†ï¼ä¸Šã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
                except ImportError as ie:
                     st.error(f"ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªä¸è¶³: {ie} (pip install openpyxl ãŒå¿…è¦ã§ã™)")
                except Exception as e:
                    st.error(f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")

            # JSON Export (For Commerce Society / Backup)
            st.divider()
            json_str = plan.model_dump_json(indent=2)
            st.download_button(
                label="ğŸ’¾ è¨ˆç”»ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ (JSON)",
                data=json_str,
                file_name=f"jigyokei_data_{plan.basic_info.corporate_name or 'plan'}.json",
                mime="application/json",
                help="å•†å·¥ä¼šé€£æºç”¨ã€ã¾ãŸã¯ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚",
                use_container_width=True
            )

        # --- 3. Section Breakdown (Application Form Style: 6 Tabs) ---
        st.divider()
        
        # Dynamic Tab Labels (6-tab structure matching electronic application)
        tabs_labels = {
            "BasicInfo": "1ï¸âƒ£ åŸºæœ¬æƒ…å ±",
            "Goals": "2ï¸âƒ£ äº‹æ¥­æ¦‚è¦ãƒ»ç›®æ¨™",
            "Disaster": "3ï¸âƒ£ ç½å®³æƒ³å®š",
            "Response": "4ï¸âƒ£ åˆå‹•å¯¾å¿œ",
            "Measures": "5ï¸âƒ£ äº‹å‰å¯¾ç­–",
            "Finance": "6ï¸âƒ£ è³‡é‡‘ãƒ»æ¨é€²ä½“åˆ¶"
        }
        
        # Check missing items to add warning icons
        missing_sections = [m['section'] for m in result['missing_mandatory']]
        
        if "BasicInfo" in missing_sections: tabs_labels["BasicInfo"] += " âš ï¸"
        if "Goals" in missing_sections: tabs_labels["Goals"] += " âš ï¸"
        if "Goals" in missing_sections: tabs_labels["Disaster"] += " âš ï¸"  # Disaster is part of Goals
        if "ResponseProcedures" in missing_sections: tabs_labels["Response"] += " âš ï¸"
        if "Measures" in missing_sections: tabs_labels["Measures"] += " âš ï¸"
        if "FinancialPlan" in missing_sections or "PDCA" in missing_sections: tabs_labels["Finance"] += " âš ï¸"

        tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
            tabs_labels["BasicInfo"], 
            tabs_labels["Goals"], 
            tabs_labels["Disaster"],
            tabs_labels["Response"],
            tabs_labels["Measures"],
            tabs_labels["Finance"]
        ])
        
        # TAB 1: Basic Info
        with tab1:
            st.caption("ğŸ“‹ æ§˜å¼ç¬¬1 åŸºæœ¬æƒ…å ±")
            if plan.basic_info:
                bi = plan.basic_info
                full_address = f"{bi.address_pref or ''}{bi.address_city or ''}{bi.address_street or ''}{bi.address_building or ''}"
                
                display_data = {
                    "ä¼šç¤¾å": bi.corporate_name,
                    "ä»£è¡¨è€…": f"{bi.representative_title or ''} {bi.representative_name or ''}".strip(),
                    "è³‡æœ¬é‡‘": f"{bi.capital:,}å††" if bi.capital else "-",
                    "å¾“æ¥­å“¡æ•°": f"{bi.employees}å" if bi.employees else "-",
                    "éƒµä¾¿ç•ªå·": bi.address_zip,
                    "ä½æ‰€": full_address,
                    "æ¥­ç¨®": f"{bi.industry_major or ''} / {bi.industry_middle or ''}".strip(" /"),
                    "æ³•äººç•ªå·": bi.corporate_number or "-"
                }
                st.table([{"é …ç›®": k, "å†…å®¹": v} for k, v in display_data.items() if v and v != "-"])
            else:
                with st.container(border=True):
                    st.warning("âš ï¸ åŸºæœ¬æƒ…å ±ãŒæœªå…¥åŠ›ã§ã™ã€‚")
        
        # TAB 2: Overview & Goals
        with tab2:
            st.caption("ğŸ“‹ æ§˜å¼ç¬¬2 äº‹æ¥­æ´»å‹•ã®æ¦‚è¦ãƒ»å–çµ„ç›®çš„")
            
            with st.container(border=True):
                st.subheader("äº‹æ¥­æ´»å‹•ã®æ¦‚è¦")
                if plan.goals.business_overview:
                    st.info(plan.goals.business_overview)
                else:
                    st.error("ğŸš¨ äº‹æ¥­æ´»å‹•ã®æ¦‚è¦ãŒæœªå…¥åŠ›ã§ã™ã€‚")
                    st.caption("è‡ªç¤¾ã®äº‹æ¥­å†…å®¹ã€ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³ä¸Šã®å½¹å‰²ã€åœ°åŸŸçµŒæ¸ˆã¸ã®è²¢çŒ®ã‚’å…·ä½“çš„ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚")
            
            with st.container(border=True):
                st.subheader("å–çµ„ç›®çš„")
                if plan.goals.business_purpose:
                    st.info(plan.goals.business_purpose)
                else:
                    st.warning("âš ï¸ å–çµ„ç›®çš„ãŒæœªå…¥åŠ›ã§ã™ã€‚")
        
        # TAB 3: Disaster Scenario
        with tab3:
            st.caption("ğŸ“‹ æ§˜å¼ç¬¬3 äº‹æ¥­æ´»å‹•ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹è‡ªç„¶ç½å®³ç­‰ã®æƒ³å®š")
            
            with st.container(border=True):
                st.subheader("æƒ³å®šã™ã‚‹è‡ªç„¶ç½å®³ç­‰")
                if plan.goals.disaster_scenario.disaster_assumption:
                    st.info(plan.goals.disaster_scenario.disaster_assumption)
                else:
                    st.error("ğŸš¨ ç½å®³æƒ³å®šãŒæœªå…¥åŠ›ã§ã™ã€‚")
                    st.caption("ãƒã‚¶ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã‚’å‚ç…§ã—ã€ã€Œéœ‡åº¦â—‹â—‹ã€ã€Œæµ¸æ°´æ·±â—‹â—‹mã€ãªã©å…·ä½“çš„ãªæ•°å€¤ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚")
            
            # New Impact Structure Display
            st.subheader("è‡ªç„¶ç½å®³ç­‰ã®ç™ºç”ŸãŒäº‹æ¥­æ´»å‹•ã«ä¸ãˆã‚‹å½±éŸ¿")
            imp = plan.goals.disaster_scenario.impacts
            impact_data = {
                "äººå“¡": imp.impact_personnel,
                "å»ºç‰©ãƒ»è¨­å‚™": imp.impact_building,
                "è³‡é‡‘ç¹°ã‚Š": imp.impact_funds,
                "æƒ…å ±": imp.impact_info
            }
            # Filter non-empty
            impact_rows = [{"é …ç›®": k, "å†…å®¹": v} for k, v in impact_data.items() if v]
            if impact_rows:
                st.table(impact_rows)
            else:
                st.warning("âš ï¸ å½±éŸ¿è©³ç´°ãŒæœªå…¥åŠ›ã§ã™ã€‚")
        
        # TAB 4: First Response
        with tab4:
            st.caption(f"ğŸ“‹ æ§˜å¼ç¬¬4 åˆå‹•å¯¾å¿œæ‰‹é †ç­‰: {len(plan.response_procedures)}ä»¶ç™»éŒ²æ¸ˆ")
            if plan.response_procedures:
                st.table([m.model_dump() for m in plan.response_procedures])
            else:
                with st.container(border=True):
                    st.error("ğŸš¨ åˆå‹•å¯¾å¿œãŒæœªç™»éŒ²ã§ã™ã€‚")
                    st.caption("ç½å®³ç™ºç”Ÿç›´å¾Œã«èª°ãŒä½•ã‚’ã™ã‚‹ã‹ï¼ˆä¾‹ï¼šå®‰å¦ç¢ºèªã€é¿é›£èª˜å°ï¼‰ã‚’æ±ºã‚ã¦ãã ã•ã„ã€‚")
        
        # TAB 5: Measures (A/B/C/D)
        with tab5:
            st.caption(f"ğŸ“‹ æ§˜å¼ç¬¬5 å¹³æ™‚ã®æ¨é€²ä½“åˆ¶ (4ã‚«ãƒ†ã‚´ãƒª)")
            
            measures = plan.measures
            
            # Helper to display MeasureDetail
            def show_measure(label, item):
                with st.expander(label, expanded=True):
                    c1, c2 = st.columns(2)
                    c1.markdown("**ç¾åœ¨ã®å–çµ„**")
                    if item.current_measure: c1.info(item.current_measure)
                    else: c1.warning("æœªå…¥åŠ›")
                    
                    c2.markdown("**ä»Šå¾Œã®è¨ˆç”»**")
                    if item.future_plan: c2.success(item.future_plan)
                    else: c2.caption("ãªã—")

            show_measure("A: äººå“¡ä½“åˆ¶ã®æ•´å‚™ (ãƒ’ãƒˆ)", measures.personnel)
            show_measure("B: å»ºç‰©ãƒ»è¨­å‚™ã®ä¿å…¨ (ãƒ¢ãƒ)", measures.building)
            show_measure("C: è³‡é‡‘èª¿é”æ‰‹æ®µã®ç¢ºä¿ (ã‚«ãƒ)", measures.money)
            show_measure("D: æƒ…å ±ã®ä¿è­· (æƒ…å ±)", measures.data)

        
        # TAB 6: Finance & PDCA
        with tab6:
            st.caption("ğŸ“‹ æ§˜å¼ç¬¬6 è³‡é‡‘è¨ˆç”»ãƒ»æ¨é€²ä½“åˆ¶")
            
            with st.container(border=True):
                st.subheader("ğŸ’° è³‡é‡‘è¨ˆç”»")
                if plan.financial_plan.items:
                    st.table([i.model_dump() for i in plan.financial_plan.items])
                else:
                    st.warning("âš ï¸ è³‡é‡‘è¨ˆç”»ãŒæœªå…¥åŠ›ã§ã™ã€‚")
                    st.caption("å¾©æ—§ã«ã‹ã‹ã‚‹è²»ç”¨ã®ç›®å®‰ã¨ã€ãã®èª¿é”æ–¹æ³•ï¼ˆä¿é™ºã€è‡ªå·±è³‡é‡‘ã€å€Ÿå…¥ãªã©ï¼‰ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
            
            with st.container(border=True):
                st.subheader("ğŸ› ï¸ è¨­å‚™ãƒªã‚¹ãƒˆ (ç¨åˆ¶å„ªé‡) (ä»»æ„)")
                if plan.equipment.items:
                    st.table([i.model_dump() for i in plan.equipment.items])
                else:
                    st.info("è¨­å‚™ãƒªã‚¹ãƒˆãªã— (ä»»æ„)")
            
            with st.container(border=True):
                st.subheader("ğŸ”„ æ¨é€²ä½“åˆ¶ãƒ»è¨“ç·´")
                pdca_data = {
                    "ç®¡ç†ä½“åˆ¶": plan.pdca.management_system or "-",
                    "è¨“ç·´ãƒ»æ•™è‚²": plan.pdca.training_education or "-"
                }
                st.table([{"é …ç›®": k, "å†…å®¹": v} for k, v in pdca_data.items()])

        # --- 4. Sidebar Tools (Injected here dynamically or rely on static layout) ---
        # Note: Sidebar is already rendered at top of script. We can add to it here or just leave as is.
        # Adding a dedicated "Tools" expander in main area for visibility
        with st.expander("ğŸ› ï¸ ãŠå½¹ç«‹ã¡ãƒ„ãƒ¼ãƒ« (External Tools)"):
            c1, c2 = st.columns(2)
            c1.link_button("ğŸŒ ãƒã‚¶ãƒ¼ãƒ‰ãƒãƒƒãƒ—", "https://disaportal.gsi.go.jp/", use_container_width=True)
            c2.link_button("ğŸ“‰ J-SHIS åœ°éœ‡äºˆæ¸¬", "https://www.j-shis.bosai.go.jp/", use_container_width=True)
            
            c3, c4 = st.columns(2)
            c3.link_button("ğŸ’´ é‡‘èæ”¯æ´ (Risk Finance)", "https://www.chusho.meti.go.jp/keiei/antei/bousai/keizokuryoku.html", use_container_width=True)
            c4.link_button("ğŸ›ï¸ ç¨åˆ¶å„ªé‡ (Tax)", "https://www.chusho.meti.go.jp/keiei/antei/bousai/keizokuryoku.html#zeisei", use_container_width=True)

    else:
        st.info("â˜ï¸ Click the button to analyze current chat history.")

    st.divider()
    with st.expander("Show Raw Chat History"):
        st.json(st.session_state.ai_interviewer.history)

elif mode == "Main Consensus Room (Resolution)":
    st.title("âš–ï¸ Consensus Room (å…¨ä½“åˆæ„)")
    st.caption("å„ãƒšãƒ«ã‚½ãƒŠã®æ„è¦‹ã‚’èª¿æ•´ã—ã€æœ€çµ‚çš„ãªæ–¹é‡ã‚’æ±ºå®šã—ã¾ã™ã€‚")

    # --- File Upload for Consensus (New) ---
    with st.expander("ğŸ“‚ è³‡æ–™ã®è¿½åŠ ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (Upload Documents)", expanded=False):
        uploaded_refs_consensus = st.file_uploader(
            "å…¨ä½“åˆæ„ç”¨è³‡æ–™ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDF/ç”»åƒ)", 
            type=["pdf", "png", "jpg", "jpeg"], 
            accept_multiple_files=True,
            key="uploader_consensus"
        )
        
        # --- Auto-Process Logic (Consensus) ---
        if "processed_file_ids" not in st.session_state:
            st.session_state.processed_file_ids = set()

        if uploaded_refs_consensus:
            new_files_to_process = []
            for file in uploaded_refs_consensus:
                file_id = f"{file.name}_{file.size}"
                if file_id not in st.session_state.processed_file_ids:
                    new_files_to_process.append(file)
                    st.session_state.processed_file_ids.add(file_id)
            
            if new_files_to_process:
                 with st.spinner("è³‡æ–™ã‚’è§£æä¸­... (Processing for Consensus)"):
                    try:
                        # Process files as "ç·åˆèª¿æ•´å½¹" (Coordinator)
                        count = st.session_state.ai_interviewer.process_files(new_files_to_process, target_persona="ç·åˆèª¿æ•´å½¹")
                        st.success(f"{count}ä»¶ã®è³‡æ–™ã‚’å…¨ä½“åˆæ„ç”¨ã«èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼")
                        
                        # Agentic Extraction Trigger (Optional but good for consistency)
                        if count > 0:
                             with st.status("ğŸ¤– AI Agent Working: è³‡æ–™ã‚’è©³ç´°åˆ†æä¸­...", expanded=True) as status:
                                 status.write("ğŸ“ Gemini 1.5 Pro (High Reasoning) ã§è³‡æ–™ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
                                 try:
                                     all_files = st.session_state.ai_interviewer.uploaded_file_refs
                                     extracted_data = st.session_state.ai_interviewer.extract_structured_data(text="", file_refs=all_files)
                                     if extracted_data:
                                         status.write("âœ… æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚")
                                     else:
                                         status.write("â„¹ï¸ æ–°è¦ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                                 except Exception as ex_e:
                                     status.error(f"Extraction Error: {ex_e}")

                        time.sleep(1)
                        perform_auto_save()
                        st.rerun()
                    except Exception as e:
                        st.error(f"èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    
    # Conflict Detection
    with st.expander("ğŸ§ çŸ›ç›¾ãƒ»æœªåˆæ„äº‹é …ã®æ¤œçŸ¥ (Conflict Detection)", expanded=True):
        if st.button("çŸ›ç›¾ã‚’å†ã‚¹ã‚­ãƒ£ãƒ³ã™ã‚‹", type="primary"):
            with st.spinner("Analyzing conflicts..."):
                conflicts = st.session_state.ai_interviewer.detect_conflicts()
                st.session_state._conflicts_cache = conflicts
        
        # Retrieve cache
        current_conflicts_data = st.session_state.get("_conflicts_cache", {})
        current_conflicts = current_conflicts_data.get("conflicts", [])
        
        if current_conflicts:
            st.warning(f"{len(current_conflicts)}ä»¶ã®çŸ›ç›¾ã¾ãŸã¯æœªåˆæ„äº‹é …ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
            for i, c in enumerate(current_conflicts):
                st.markdown(f"#### {i+1}. {c.get('topic', 'Topic')}")
                col1, col2 = st.columns(2)
                with col1:
                    st.info(f"**A: {c.get('persona_A')}**\n\n{c.get('statement_A')}")
                with col2:
                    st.info(f"**B: {c.get('persona_B')}**\n\n{c.get('statement_B')}")
                st.success(f"ğŸ’¡ **AI Suggestion**: {c.get('suggestion')}")
                st.divider()
        else:
             st.info("çŸ›ç›¾ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ (æœªã‚¹ã‚­ãƒ£ãƒ³ã¾ãŸã¯è§£æ¶ˆæ¸ˆã¿)")

    st.divider()
    st.subheader("ğŸ’¬ å…¨ä½“æ–¹é‡ã®æ±ºå®š")
    
    # Chat History
    history = st.session_state.ai_interviewer.history
    
    # Helper for rendering messages in Consensus Mode (Duplicate of Chat Mode helper to avoid scope issues)
    def render_message_consensus(msg, current_persona):
        if not isinstance(msg, dict): return
        role = msg["role"]
        msg_persona = msg.get("persona", "Unknown")
        target_persona = msg.get("target_persona")
        
        # In Consensus, we generally want to see everything, OR filter by "General" context.
        # However, to be safe and match user expectation: show messages relevant to 'ç·åˆèª¿æ•´å½¹' or public.
        # Let's show everything for now as it is a "Consensus" room.
        # But if we want to be strict: 
        visible = True # Default to visible in Consensus
        # Apply filter if needed:
        # if role == "model" and target_persona and target_persona != "ç·åˆèª¿æ•´å½¹": visible = False
        
        if visible:
            avatar = "ğŸ¤–" if role == "model" else "ğŸ‘¤"
            if msg_persona == "çµŒå–¶è€…": avatar = "ğŸ‘¨â€ğŸ’¼"
            elif msg_persona == "å¾“æ¥­å“¡": avatar = "ğŸ‘·"
            elif msg_persona == "å•†å·¥ä¼šè·å“¡": avatar = "ğŸ§‘â€ğŸ«"
            elif msg_persona == "AI Concierge": avatar = "ğŸ¤–"
            elif msg_persona == "ç·åˆèª¿æ•´å½¹": avatar = "âš–ï¸"
            
            with st.chat_message(role, avatar=avatar):
                st.caption(f"{msg_persona}")
                
                # Sanitize content
                import re
                content = msg["content"]
                
                # 1. Hide <suggestions> block
                content = re.sub(r'<suggestions>.*?</suggestions>', '', content, flags=re.DOTALL).strip()
                
                # 2. Hide Raw JSON System Notification
                if "ã€ã‚·ã‚¹ãƒ†ãƒ é€šçŸ¥: æ—¢å­˜è¨ˆç”»ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã€‘" in content:
                    import json
                    try:
                         json_match = re.search(r'```json\n(.*?)\n```', content, flags=re.DOTALL)
                         if json_match:
                             data = json.loads(json_match.group(1))
                             c_name = data.get("basic_info", {}).get("corporate_name", "Unknown")
                             st.success(f"âœ… æ—¢å­˜ã®äº‹æ¥­è¨ˆç”»ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ (å¯¾è±¡ä¼æ¥­: {c_name})")
                             with st.expander("è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèª (View Raw Data)"):
                                 st.json(data)
                             return
                    except:
                        pass

                # 3. Final Confirmation Block (<verify>)
                verify_match = re.search(r'<verify>(.*?)</verify>', content, flags=re.DOTALL)
                if verify_match:
                    # Render the verification block in a specific colored container
                    verify_text = verify_match.group(1).strip()
                    # Remove the verify block from main content to avoid double rendering
                    main_text = re.sub(r'<verify>.*?</verify>', '', content, flags=re.DOTALL).strip()
                    
                    if main_text:
                        st.markdown(main_text)
                    
                    # Render Verification Card
                    with st.container(border=True):
                        st.info("ğŸ“‹ **ä»¥ä¸‹ã®å†…å®¹ã§ç™»éŒ²ã—ã¾ã™ã€‚ç¢ºèªã‚’ãŠé¡˜ã„ã—ã¾ã™**")
                        st.markdown(verify_text) # Markdown inside supports bold etc.
                        
                else:
                    # Normal render
                    st.markdown(content)

    # Move Recommended Actions to Main Area (Expander at Top)
    with st.expander("ğŸ’¡ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (Recommended Actions)", expanded=True):
        # 1. From Consensus Chat Suggestions
        if "_consensus_suggestions" in st.session_state:
            sugg = st.session_state._consensus_suggestions
            if "options" in sugg and sugg["options"]:
                st.caption("ğŸ¤– AIã‹ã‚‰ã®ææ¡ˆ:")
                cols = st.columns(len(sugg["options"]))
                for i, opt in enumerate(sugg["options"]):
                    cols[i].info(f"ğŸ‘‰ {opt}")
        
        # 2. From Missing Items (Static Analysis)
        if "current_plan" in st.session_state and st.session_state.current_plan:
             from src.core.completion_checker import CompletionChecker
             result = CompletionChecker.analyze(st.session_state.current_plan)
             if result["missing_mandatory"]:
                 st.caption("âš ï¸ æœªå®šã®å¿…é ˆé …ç›®:")
                 for m in result["missing_mandatory"][:3]: 
                     st.warning(f"ğŸ“Œ {m['section']}")
        else:
            st.caption("â„¹ï¸ è¨ˆç”»ãƒ‡ãƒ¼ã‚¿æœªèª­ã¿è¾¼ã¿")

    # Show history using rendered helper
    for i in range(len(history)):
         render_message_consensus(history[i], "ç·åˆèª¿æ•´å½¹") 
    
    # Input
    
    # --- Mini Progress Dashboard (Placeholder) ---
    dashboard_placeholder = st.empty()

    def render_consensus_dashboard(placeholder):
        if "current_plan" in st.session_state and st.session_state.current_plan:
             # Logic is same, but inside container
             with placeholder.container():
                from src.core.completion_checker import CompletionChecker
                res = CompletionChecker.analyze(st.session_state.current_plan)
                prog = res['mandatory_progress']
                
                cols_prog = st.columns([3, 1])
                with cols_prog[0]: st.progress(prog)
                with cols_prog[1]: st.caption(f"**{int(prog*100)}% å®Œäº†**")
                
                if res['missing_mandatory']:
                    sec_map = {"BasicInfo": "åŸºæœ¬æƒ…å ±", "Goals": "äº‹æ¥­æ¦‚è¦", "Disaster": "ç½å®³æƒ³å®š", "ResponseProcedures": "åˆå‹•å¯¾å¿œ", "Measures": "äº‹å‰å¯¾ç­–", "FinancialPlan": "è³‡é‡‘è¨ˆç”»", "PDCA": "æ¨é€²ä½“åˆ¶"}
                    next_items = [sec_map.get(m['section'], m['section']) for m in res['missing_mandatory'][:3]]
                    st.write("ğŸ“Œ **æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³:** " + "  ".join([f"`{item}`" for item in next_items]))

    render_consensus_dashboard(dashboard_placeholder)

    if prompt := st.chat_input("å…¨ä½“æ–¹é‡ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (ä¾‹: é¿é›£å ´æ‰€ã¯é«˜å°ã®å…¬åœ’ã¨ã—ã¾ã™)"):
         with st.chat_message("user", avatar="ğŸ‘¤"):
            st.markdown(prompt)

         # Send Message
         user_data = {"name": "Consensus", "position": "Manager"}
         with st.chat_message("model", avatar="ğŸ¤–"):
             with st.spinner("èª¿æ•´ä¸­..."):
                response = st.session_state.ai_interviewer.send_message(
                    prompt, 
                    persona="å…¨ä½“åˆæ„",
                    user_data=user_data
                )
                
                # --- Incremental Plan Analysis (Consensus) ---
                import re
                update_match = re.search(r'<update>(.*?)</update>', response, flags=re.DOTALL)
                
                if update_match:
                    try:
                        update_json_str = update_match.group(1).strip()
                        update_data = json.loads(update_json_str)
                        if apply_incremental_update(update_data):
                            # Update dashboard in-place
                            render_consensus_dashboard(dashboard_placeholder)
                            st.toast("âš¡ å…¨ä½“æ–¹é‡ã‚’åæ˜ ã—ã¾ã—ãŸ", icon="âœ…")
                    except Exception as e:
                        print(f"Update Parse Failed: {e}")
                
                # Strip and display
                display_response = re.sub(r'<update>.*?</update>', '', response, flags=re.DOTALL).strip()
                st.markdown(display_response)

                perform_auto_save()
                # NO RERUN
